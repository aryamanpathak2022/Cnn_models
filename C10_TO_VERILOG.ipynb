{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryamanpathak2022/Cnn_models/blob/main/C10_TO_VERILOG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GM5DrWoXJME",
        "outputId": "94d63f15-1e89-48cb-dd2b-c9568eb527e6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nngen in /usr/local/lib/python3.10/dist-packages (1.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: veriloggen>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from nngen) (2.3.0)\n",
            "Requirement already satisfied: onnx>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from nngen) (1.16.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.9.0->nngen) (3.20.3)\n",
            "Requirement already satisfied: pyverilog>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from veriloggen>=2.3.0->nngen) (1.3.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from pyverilog>=1.3.0->veriloggen>=2.3.0->nngen) (3.1.4)\n",
            "Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.10/dist-packages (from pyverilog>=1.3.0->veriloggen>=2.3.0->nngen) (3.11)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->pyverilog>=1.3.0->veriloggen>=2.3.0->nngen) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nngen numpy\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PYTORCH MODEL"
      ],
      "metadata": {
        "id": "4EexKbBUbUj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ComplexCNN(nn.Module):\n",
        "     def __init__(self):\n",
        "        super(ComplexCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # (N, 3, 32, 32) -> (N, 64, 32, 32)\n",
        "        self.bn1 = nn.BatchNorm2d(64)                          # (N, 64, 32, 32) -> (N, 64, 32, 32)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # (N, 64, 32, 32) -> (N, 128, 32, 32)\n",
        "        self.bn2 = nn.BatchNorm2d(128)                         # (N, 128, 32, 32) -> (N, 128, 32, 32)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # (N, 128, 32, 32) -> (N, 256, 32, 32)\n",
        "        self.bn3 = nn.BatchNorm2d(256)                         # (N, 256, 32, 32) -> (N, 256, 32, 32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)                         # (N, 256, 32, 32) -> (N, 256, 16, 16)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)                # (N, 256 * 4 * 4) -> (N, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)                        # (N, 1024) -> (N, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)                          # (N, 512) -> (N, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "     def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))         # (N, 3, 32, 32) -> (N, 64, 32, 32) -> (N, 64, 16, 16)\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))         # (N, 64, 16, 16) -> (N, 128, 16, 16) -> (N, 128, 8, 8)\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))         # (N, 128, 8, 8) -> (N, 256, 8, 8) -> (N, 256, 4, 4)\n",
        "        x = x.view(-1, 256 * 4 * 4)                            # (N, 256, 4, 4) -> (N, 256 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))                                # (N, 256 * 4 * 4) -> (N, 1024)\n",
        "        x = self.dropout(x)                                    # (N, 1024) -> (N, 1024)\n",
        "        x = F.relu(self.fc2(x))                                # (N, 1024) -> (N, 512)\n",
        "        x = self.fc3(x)                                        # (N, 512) -> (N, 10)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5leHEzDxbUFH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NNGEN MODEL DEFINATION\n"
      ],
      "metadata": {
        "id": "bYsW5RqTaJ53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import nngen as ng\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "#  NNgen data types\n",
        "act_dtype = ng.int16\n",
        "weight_dtype = ng.int16\n",
        "bias_dtype = ng.int16\n",
        "scale_dtype = ng.int16\n",
        "\n",
        "# Input placeholder\n",
        "input_layer = ng.placeholder(dtype=act_dtype, shape=(1, 32, 32, 3), name='input_layer')\n",
        "\n",
        "# Layer 0: conv2d, batchnorm, relu, max_pool\n",
        "w0 = ng.variable(dtype=weight_dtype, shape=(64, 3, 3, 3), name='w0')\n",
        "b0 = ng.variable(dtype=bias_dtype, shape=(w0.shape[0],), name='b0')\n",
        "s0 = ng.variable(dtype=scale_dtype, shape=(w0.shape[0],), name='s0')\n",
        "\n",
        "a0 = ng.conv2d(input_layer, w0, strides=(1, 1, 1, 1), bias=b0, scale=s0, act_func=ng.relu, sum_dtype=ng.int64)\n",
        "\n",
        "a0 = ng.max_pool_serial(a0, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1))\n",
        "\n",
        "# Layer 1: conv2d, batchnorm, relu, max_pool\n",
        "w1 = ng.variable(weight_dtype, shape=(128, 3, 3, a0.shape[-1]), name='w1')\n",
        "b1 = ng.variable(bias_dtype, shape=(w1.shape[0],), name='b1')\n",
        "s1 = ng.variable(scale_dtype, shape=(w1.shape[0],), name='s1')\n",
        "\n",
        "a1 = ng.conv2d(a0, w1, strides=(1, 1, 1, 1), bias=b1, scale=s1, act_func=ng.relu, sum_dtype=ng.int64)\n",
        "\n",
        "a1 = ng.max_pool_serial(a1, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1))\n",
        "\n",
        "# Layer 2: conv2d, batchnorm, relu, max_pool\n",
        "w2 = ng.variable(weight_dtype, shape=(256, 3, 3, a1.shape[-1]), name='w2')\n",
        "b2 = ng.variable(bias_dtype, shape=(w2.shape[0],), name='b2')\n",
        "s2 = ng.variable(scale_dtype, shape=(w2.shape[0],), name='s2')\n",
        "\n",
        "a2 = ng.conv2d(a1, w2, strides=(1, 1, 1, 1), bias=b2, scale=s2, act_func=ng.relu, sum_dtype=ng.int64)\n",
        "a2 = ng.max_pool_serial(a2, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1))\n",
        "\n",
        "# Flatten the output for fully connected layers\n",
        "a2_flat = ng.reshape(a2, [1, -1])\n",
        "\n",
        "# Layer 3: full-connection, relu\n",
        "w3 = ng.variable(weight_dtype, shape=(1024, a2_flat.shape[-1]), name='w3')\n",
        "b3 = ng.variable(bias_dtype, shape=(w3.shape[0],), name='b3')\n",
        "s3 = ng.variable(scale_dtype, shape=(w3.shape[0],), name='s3')\n",
        "\n",
        "a3 = ng.matmul(a2_flat, w3, bias=b3, scale=s3, transposed_b=True, act_func=ng.relu, sum_dtype=ng.int64)\n",
        "\n",
        "# Layer 4: full-connection, relu\n",
        "w4 = ng.variable(weight_dtype, shape=(512, a3.shape[-1]), name='w4')\n",
        "b4 = ng.variable(bias_dtype, shape=(w4.shape[0],), name='b4')\n",
        "s4 = ng.variable(scale_dtype, shape=(w4.shape[0],), name='s4')\n",
        "\n",
        "a4 = ng.matmul(a3, w4, bias=b4, scale=s4, transposed_b=True, act_func=ng.relu, sum_dtype=ng.int64)\n",
        "\n",
        "# Layer 5: full-connection (output layer)\n",
        "w5 = ng.variable(weight_dtype, shape=(10, a4.shape[-1]), name='w5')\n",
        "b5 = ng.variable(bias_dtype, shape=(w5.shape[0],), name='b5')\n",
        "s5 = ng.variable(scale_dtype, shape=(w5.shape[0],), name='s5')\n",
        "print(w5.value)\n",
        "\n",
        "output_layer = ng.matmul(a4, w5, bias=b5, scale=s5, transposed_b=True, name='output_layer', sum_dtype=ng.int64)\n"
      ],
      "metadata": {
        "id": "iGjOxTucaMLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087d0c13-7597-4065-8d4c-a2e9459e68cc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEIGHTS"
      ],
      "metadata": {
        "id": "JM8k07z0bKZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming you have a trained PyTorch model\n",
        "pytorch_model = ComplexCNN()\n",
        "pytorch_model.load_state_dict(torch.load('cifar_net.pth'))  # Loading  trained model parameters\n",
        "\n",
        "# Assigning  weights to NNgen variables\n",
        "w0.set_value(pytorch_model.conv1.weight.data.numpy().astype(np.int64))\n",
        "b0.set_value(pytorch_model.bn1.bias.data.numpy().astype(np.int64))\n",
        "s0.set_value(np.ones(s0.shape, dtype=np.int64))\n",
        "\n",
        "w1.set_value(pytorch_model.conv2.weight.data.numpy().astype(np.int64))\n",
        "b1.set_value(pytorch_model.bn2.bias.data.numpy().astype(np.int64))\n",
        "s1.set_value(np.ones(s1.shape, dtype=np.int64))\n",
        "\n",
        "w2.set_value(pytorch_model.conv3.weight.data.numpy().astype(np.int64))\n",
        "b2.set_value(pytorch_model.bn3.bias.data.numpy().astype(np.int64))\n",
        "s2.set_value(np.ones(s2.shape, dtype=np.int64))\n",
        "\n",
        "w3.set_value(pytorch_model.fc1.weight.data.numpy().astype(np.int64).transpose())\n",
        "b3.set_value(pytorch_model.fc1.bias.data.numpy().astype(np.int64))\n",
        "s3.set_value(np.ones(s3.shape, dtype=np.int64))\n",
        "\n",
        "w4.set_value(pytorch_model.fc2.weight.data.numpy().astype(np.int64).transpose())\n",
        "b4.set_value(pytorch_model.fc2.bias.data.numpy().astype(np.int64))\n",
        "s4.set_value(np.ones(s4.shape, dtype=np.int64))\n",
        "\n",
        "w5.set_value(pytorch_model.fc3.weight.data.numpy().astype(np.int64).transpose())\n",
        "b5.set_value(pytorch_model.fc3.bias.data.numpy().astype(np.int64))\n",
        "s5.set_value(np.ones(s5.shape, dtype=np.int64))\n",
        "\n",
        "print(output_layer.value)\n"
      ],
      "metadata": {
        "id": "PCHnEVKxbMgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45038222-926b-4bf6-87ab-98ef8d43a7f4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrJ2CMIyv9Ez",
        "outputId": "e3290b74-ebb9-4e90-96c7-bbddd64e557a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_model = ComplexCNN()\n",
        "pytorch_model.load_state_dict(torch.load('cifar_net.pth'))\n",
        "\n",
        "# Print the weights to ensure they are loaded correctly\n",
        "state_dict = pytorch_model.state_dict()\n",
        "for name, param in state_dict.items():\n",
        "    print(f\"{name}: {param}\")\n",
        "\n",
        "# Ensure that the weights are not zero after loading\n",
        "print(\"Checking conv1 weights after loading:\")\n",
        "print(pytorch_model.conv1.weight.data)\n",
        "\n",
        "# Assign quantized weights to the NNgen variables\n",
        "def quantize_weights(weight_data, width):\n",
        "    weight_data = np.clip(weight_data, -5.0, 5.0)\n",
        "    weight_data = weight_data * (2.0 ** (width - 1) - 1) / 5.0\n",
        "    return np.round(weight_data).astype(np.int64)\n",
        "\n",
        "# Example quantization for conv1 weights\n",
        "w0_value = quantize_weights(pytorch_model.conv1.weight.data.numpy(), 8)\n",
        "print(\"Quantized conv1 weights:\")\n",
        "print(w0_value)\n",
        "\n",
        "# Assign values to NNgen variables\n",
        "w0.set_value(w0_value)\n",
        "b0.set_value(pytorch_model.bn1.bias.data.numpy().astype(np.int64))\n",
        "s0.set_value(np.ones(s0.shape, dtype=np.int64))\n",
        "\n",
        "# Similarly, assign and check weights for other layers\n",
        "w1.set_value(quantize_weights(pytorch_model.conv2.weight.data.numpy(), 8))\n",
        "b1.set_value(pytorch_model.bn2.bias.data.numpy().astype(np.int64))\n",
        "s1.set_value(np.ones(s1.shape, dtype=np.int64))\n",
        "\n",
        "w2.set_value(quantize_weights(pytorch_model.conv3.weight.data.numpy(), 8))\n",
        "b2.set_value(pytorch_model.bn3.bias.data.numpy().astype(np.int64))\n",
        "s2.set_value(np.ones(s2.shape, dtype=np.int64))\n",
        "\n",
        "w3.set_value(quantize_weights(pytorch_model.fc1.weight.data.numpy(), 8).transpose())\n",
        "b3.set_value(pytorch_model.fc1.bias.data.numpy().astype(np.int64))\n",
        "s3.set_value(np.ones(s3.shape, dtype=np.int64))\n",
        "\n",
        "w4.set_value(quantize_weights(pytorch_model.fc2.weight.data.numpy(), 8).transpose())\n",
        "b4.set_value(pytorch_model.fc2.bias.data.numpy().astype(np.int64))\n",
        "s4.set_value(np.ones(s4.shape, dtype=np.int64))\n",
        "\n",
        "w5.set_value(quantize_weights(pytorch_model.fc3.weight.data.numpy(), 8).transpose())\n",
        "b5.set_value(pytorch_model.fc3.bias.data.numpy().astype(np.int64))\n",
        "s5.set_value(np.ones(s5.shape, dtype=np.int64))\n",
        "\n",
        "# Check the NNgen variables\n",
        "print(\"NNgen variable w0 values:\")\n",
        "print(w0.value)"
      ],
      "metadata": {
        "id": "jd0Ujn_DQpN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14c6053-bf35-446d-98e6-7b572984fcd3",
        "collapsed": true
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight: tensor([[[[ 0.1532,  0.2262,  0.1518],\n",
            "          [-0.1939, -0.2444, -0.0565],\n",
            "          [-0.1329, -0.2885, -0.1596]],\n",
            "\n",
            "         [[-0.0299,  0.2585,  0.1343],\n",
            "          [-0.1591,  0.1495,  0.2431],\n",
            "          [-0.1123, -0.0622, -0.0253]],\n",
            "\n",
            "         [[-0.0038, -0.0171,  0.0136],\n",
            "          [ 0.0598,  0.1440, -0.1341],\n",
            "          [ 0.1565, -0.1545,  0.0560]]],\n",
            "\n",
            "\n",
            "        [[[-0.0560, -0.2677, -0.0152],\n",
            "          [-0.3005, -0.2657, -0.2643],\n",
            "          [-0.1437, -0.1792, -0.2201]],\n",
            "\n",
            "         [[ 0.1807,  0.3710,  0.0316],\n",
            "          [ 0.4806,  0.6360,  0.4813],\n",
            "          [ 0.2348,  0.3678,  0.3855]],\n",
            "\n",
            "         [[-0.0407, -0.2538,  0.0169],\n",
            "          [-0.3217, -0.2495, -0.3543],\n",
            "          [-0.0284, -0.4016, -0.1158]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0054,  0.0565,  0.0595],\n",
            "          [ 0.1971, -0.0149, -0.2864],\n",
            "          [-0.0724,  0.1028, -0.0829]],\n",
            "\n",
            "         [[ 0.1058,  0.2043, -0.2594],\n",
            "          [ 0.2741, -0.0642, -0.1653],\n",
            "          [ 0.1833, -0.0568, -0.2118]],\n",
            "\n",
            "         [[ 0.2582, -0.0627, -0.1724],\n",
            "          [ 0.0827,  0.0415, -0.1085],\n",
            "          [ 0.2359, -0.1181, -0.2380]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.3017,  0.2462,  0.1065],\n",
            "          [ 0.3258, -0.2903, -0.2470],\n",
            "          [-0.0052, -0.3644, -0.3301]],\n",
            "\n",
            "         [[ 0.0238, -0.2379, -0.0451],\n",
            "          [-0.0269, -0.1607,  0.2029],\n",
            "          [ 0.1282,  0.0504,  0.1970]],\n",
            "\n",
            "         [[-0.1761, -0.1161, -0.0581],\n",
            "          [-0.1644, -0.0028,  0.1290],\n",
            "          [-0.0969,  0.0391,  0.3500]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1231, -0.1953, -0.1040],\n",
            "          [ 0.1350,  0.1959, -0.2623],\n",
            "          [-0.0322,  0.1045,  0.1441]],\n",
            "\n",
            "         [[-0.0161, -0.1821,  0.0909],\n",
            "          [ 0.1384,  0.1617, -0.2822],\n",
            "          [-0.1715,  0.2259,  0.0616]],\n",
            "\n",
            "         [[-0.1019, -0.0985, -0.0917],\n",
            "          [ 0.2044, -0.0564, -0.3139],\n",
            "          [ 0.0398,  0.3717, -0.0666]]],\n",
            "\n",
            "\n",
            "        [[[-0.1340,  0.1684,  0.1261],\n",
            "          [ 0.0301,  0.1927,  0.0225],\n",
            "          [ 0.1631, -0.1491, -0.2873]],\n",
            "\n",
            "         [[ 0.1753,  0.1475,  0.1032],\n",
            "          [ 0.0970, -0.0266,  0.0565],\n",
            "          [ 0.1466, -0.1547, -0.1612]],\n",
            "\n",
            "         [[ 0.1148, -0.2044, -0.0840],\n",
            "          [-0.2260,  0.0216, -0.1739],\n",
            "          [-0.0629, -0.1477, -0.2780]]]])\n",
            "conv1.bias: tensor([ 0.0933,  0.0740,  0.0428, -0.0508, -0.0494,  0.1101, -0.1711,  0.1821,\n",
            "        -0.0298, -0.0801, -0.0764, -0.1889, -0.1527, -0.1819, -0.0856, -0.1428,\n",
            "        -0.1825,  0.1002, -0.0570,  0.0708,  0.1269,  0.1354, -0.0635,  0.1301,\n",
            "         0.1555,  0.1867, -0.0967,  0.1013,  0.0264,  0.0206,  0.0644, -0.0647,\n",
            "         0.0644, -0.1534,  0.1312,  0.0328,  0.0543,  0.0149,  0.0810,  0.1336,\n",
            "         0.1763, -0.0136, -0.0426, -0.1895,  0.0610,  0.0825,  0.0085,  0.0837,\n",
            "         0.0939,  0.1764, -0.1634, -0.1226,  0.1793, -0.0947, -0.0985, -0.0057,\n",
            "        -0.0871,  0.0604, -0.1642, -0.1744, -0.0022,  0.1426, -0.0156, -0.0958])\n",
            "bn1.weight: tensor([0.9467, 1.0513, 1.0188, 1.1585, 0.9601, 0.8378, 0.9879, 0.9800, 1.1063,\n",
            "        1.0106, 1.0956, 1.0021, 1.1336, 1.1265, 0.9774, 1.0333, 0.9497, 1.2447,\n",
            "        0.8762, 0.8764, 0.9508, 0.8587, 0.9090, 1.0686, 1.2405, 0.8065, 1.1414,\n",
            "        0.7919, 1.0399, 1.0175, 0.9065, 0.8901, 0.8814, 0.9021, 0.9373, 0.9909,\n",
            "        1.0049, 0.9625, 1.1685, 1.0791, 0.9430, 1.0550, 0.7762, 0.8672, 1.0936,\n",
            "        0.9392, 0.9171, 1.1706, 1.1679, 0.9464, 0.9171, 0.8981, 1.0470, 0.9503,\n",
            "        0.9508, 0.8404, 1.0934, 1.0027, 0.9799, 0.9752, 0.8922, 0.9905, 1.0260,\n",
            "        1.0384])\n",
            "bn1.bias: tensor([-0.0388,  0.0880, -0.1026, -0.1343, -0.2389, -0.1039, -0.0544, -0.0938,\n",
            "        -0.0359, -0.0297, -0.0592, -0.1165, -0.1009, -0.0413,  0.0402, -0.0415,\n",
            "        -0.2245,  0.0198, -0.0103, -0.2341,  0.0144, -0.2272, -0.2575,  0.0352,\n",
            "        -0.1464, -0.0196,  0.0216, -0.0036, -0.0005, -0.0174, -0.1274, -0.1862,\n",
            "        -0.0803, -0.1669, -0.0181, -0.3091, -0.0366, -0.0400, -0.0559, -0.1060,\n",
            "        -0.1355, -0.0032,  0.1381, -0.1996, -0.0108, -0.0664,  0.0015, -0.0556,\n",
            "         0.0137, -0.2155, -0.2047, -0.1808, -0.0825,  0.0066, -0.0473, -0.1730,\n",
            "         0.0947, -0.0543, -0.1997,  0.1404, -0.2606,  0.0100, -0.0409, -0.0923])\n",
            "bn1.running_mean: tensor([ 0.1079,  0.0645,  0.0440, -0.0338, -0.0636,  0.1595, -0.1918,  0.1870,\n",
            "        -0.0313, -0.0861, -0.0721, -0.1839, -0.1500, -0.1795, -0.0778, -0.1374,\n",
            "        -0.1881,  0.1079, -0.0607,  0.0737,  0.1229,  0.1341, -0.0500,  0.1364,\n",
            "         0.1567,  0.2068, -0.1009,  0.1349,  0.0115,  0.0091,  0.0561, -0.0507,\n",
            "         0.0751, -0.1606,  0.1466, -0.0019,  0.0543,  0.0072,  0.0800,  0.1261,\n",
            "         0.1700, -0.0194, -0.0014, -0.1971,  0.0507,  0.0850,  0.0042,  0.0840,\n",
            "         0.0883,  0.1793, -0.1858, -0.1371,  0.1939, -0.0931, -0.0860,  0.0271,\n",
            "        -0.0838,  0.0580, -0.1676, -0.1789,  0.0084,  0.1376, -0.0246, -0.1127])\n",
            "bn1.running_var: tensor([0.5001, 1.0494, 0.8797, 0.6511, 0.1665, 1.4383, 0.7887, 0.6293, 0.7993,\n",
            "        0.4809, 0.9576, 0.2729, 0.2466, 0.5578, 0.9798, 0.4662, 0.4748, 1.0961,\n",
            "        0.1632, 0.7714, 0.8503, 0.3169, 0.6363, 0.5831, 1.0296, 0.2802, 0.5326,\n",
            "        1.0868, 0.2493, 1.0228, 0.4154, 0.3673, 1.6535, 1.8344, 0.4422, 0.8468,\n",
            "        0.5188, 0.6344, 1.4025, 0.3990, 0.6051, 0.9087, 0.9330, 0.7459, 0.8978,\n",
            "        0.2429, 0.3977, 0.4938, 0.2262, 0.4496, 1.7840, 0.7761, 0.9347, 0.3877,\n",
            "        0.5383, 0.9341, 0.3906, 0.4989, 0.4816, 0.9149, 0.6783, 0.2255, 0.4820,\n",
            "        0.9830])\n",
            "bn1.num_batches_tracked: 5100\n",
            "conv2.weight: tensor([[[[-4.0651e-02, -2.4470e-02, -1.1485e-02],\n",
            "          [ 1.3072e-02, -2.9182e-02, -7.9453e-03],\n",
            "          [-9.2399e-03,  2.6526e-02,  4.5806e-02]],\n",
            "\n",
            "         [[ 1.2679e-02,  1.0307e-01,  8.5777e-02],\n",
            "          [-5.0117e-02, -6.7858e-02,  1.9639e-02],\n",
            "          [-5.9596e-02, -1.0344e-01, -2.2390e-02]],\n",
            "\n",
            "         [[ 1.7120e-02, -2.6428e-02,  8.1993e-03],\n",
            "          [ 2.6887e-02,  3.7655e-02,  4.3002e-02],\n",
            "          [ 3.6245e-02,  1.7747e-02,  1.0424e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.1967e-02, -2.0121e-03,  7.2186e-02],\n",
            "          [ 5.4588e-02,  4.2294e-02, -5.4717e-03],\n",
            "          [ 1.7417e-02, -2.5597e-02, -2.0023e-02]],\n",
            "\n",
            "         [[ 1.4833e-03,  2.2165e-02,  1.7146e-02],\n",
            "          [-3.6887e-02, -1.8767e-02, -1.4765e-02],\n",
            "          [ 2.4033e-03,  3.8822e-02,  1.1914e-02]],\n",
            "\n",
            "         [[ 4.4027e-02,  1.7043e-02,  5.3741e-03],\n",
            "          [-7.4350e-02, -8.8006e-02, -2.9505e-02],\n",
            "          [-5.8732e-02, -5.2083e-02,  2.0230e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4174e-02,  4.3155e-03,  2.7452e-02],\n",
            "          [-5.3543e-03, -4.2589e-02,  8.3775e-03],\n",
            "          [ 1.1968e-02,  1.8758e-02,  1.3462e-02]],\n",
            "\n",
            "         [[-7.5869e-03, -4.6278e-02,  3.2174e-02],\n",
            "          [ 6.3809e-03, -7.9278e-02,  3.1481e-02],\n",
            "          [ 4.9223e-02, -6.0985e-02,  4.0085e-02]],\n",
            "\n",
            "         [[ 1.6333e-02, -2.1790e-02, -6.5632e-02],\n",
            "          [ 3.4340e-02, -4.5202e-02, -3.0894e-02],\n",
            "          [-2.2224e-02, -2.0481e-02, -2.1254e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.8609e-02,  1.2912e-02,  3.3672e-02],\n",
            "          [-1.6432e-02,  5.4091e-02,  1.0809e-02],\n",
            "          [-2.9856e-02,  7.6543e-02, -2.6364e-02]],\n",
            "\n",
            "         [[-2.7136e-02, -4.7195e-02,  6.9975e-03],\n",
            "          [-1.6124e-02, -4.6859e-02, -6.8732e-03],\n",
            "          [-5.8880e-03,  3.7942e-02, -1.8508e-02]],\n",
            "\n",
            "         [[-6.8231e-03, -2.6770e-02,  3.4783e-02],\n",
            "          [ 3.2525e-02,  4.8981e-03,  5.0183e-02],\n",
            "          [ 6.1830e-02,  4.2338e-02,  4.6787e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4964e-02,  1.2333e-01,  4.2527e-02],\n",
            "          [-3.9555e-03,  2.1387e-02,  2.5298e-02],\n",
            "          [ 6.6020e-03, -9.3790e-03,  5.0887e-02]],\n",
            "\n",
            "         [[ 3.7324e-02,  1.4422e-02, -2.6193e-03],\n",
            "          [-5.9031e-02, -1.1205e-01, -1.3891e-02],\n",
            "          [-9.2925e-02, -9.2362e-02, -8.4563e-02]],\n",
            "\n",
            "         [[-8.8330e-03, -9.1295e-02, -3.7459e-02],\n",
            "          [-3.6376e-04,  9.1091e-03, -4.6089e-02],\n",
            "          [ 1.4715e-02, -9.3531e-03,  5.1968e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.1226e-02, -2.1184e-02, -2.4260e-02],\n",
            "          [ 6.8843e-02,  8.6579e-03, -1.5773e-02],\n",
            "          [-1.4351e-02, -1.2510e-02,  2.4603e-02]],\n",
            "\n",
            "         [[-2.1521e-02, -4.3131e-03,  3.9927e-02],\n",
            "          [ 7.1801e-04, -2.4856e-02,  3.1849e-02],\n",
            "          [ 5.5375e-02,  1.6350e-02, -5.8906e-02]],\n",
            "\n",
            "         [[ 2.8390e-02,  8.6220e-03,  1.4414e-02],\n",
            "          [-3.6966e-02, -9.7665e-02, -8.8910e-03],\n",
            "          [-1.9115e-02, -6.0337e-02, -2.4602e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 4.7383e-04, -2.6736e-02,  2.8682e-02],\n",
            "          [-4.2450e-02,  3.4309e-03, -1.5109e-02],\n",
            "          [ 1.6255e-02, -3.2986e-02,  1.3197e-02]],\n",
            "\n",
            "         [[-1.1607e-01, -3.4907e-02,  6.7573e-02],\n",
            "          [-6.7352e-02, -9.8978e-03,  8.0115e-02],\n",
            "          [ 1.2025e-02,  9.6108e-04,  7.8876e-02]],\n",
            "\n",
            "         [[ 3.8451e-02, -4.9453e-02,  1.7004e-02],\n",
            "          [-1.3488e-02, -3.9503e-03, -2.3178e-02],\n",
            "          [-1.8102e-04, -1.0999e-02,  1.1039e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.1123e-02,  3.3876e-03, -5.5193e-02],\n",
            "          [ 5.2572e-02, -3.6409e-02, -2.7959e-02],\n",
            "          [-2.7461e-02, -4.9897e-02,  2.0061e-02]],\n",
            "\n",
            "         [[-9.2627e-02,  6.3539e-02, -2.6593e-02],\n",
            "          [-2.5811e-02,  5.8963e-02, -7.1510e-02],\n",
            "          [ 6.6066e-02, -2.8746e-03, -2.9654e-02]],\n",
            "\n",
            "         [[ 1.7910e-02, -1.0232e-01, -3.3907e-02],\n",
            "          [ 2.3037e-03,  2.1101e-02,  1.0860e-04],\n",
            "          [-7.7963e-03,  3.4173e-02,  1.9500e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.9127e-02,  7.3904e-03, -1.8871e-02],\n",
            "          [-6.2480e-03, -4.5156e-02, -5.7889e-02],\n",
            "          [-6.5294e-02, -5.8097e-02, -1.1459e-02]],\n",
            "\n",
            "         [[ 2.8668e-03, -7.6269e-03, -2.9474e-02],\n",
            "          [-4.3609e-02, -6.5526e-02, -7.8388e-02],\n",
            "          [-4.8875e-02, -5.1664e-02, -2.2635e-02]],\n",
            "\n",
            "         [[-1.9363e-03,  2.3595e-02,  5.5133e-02],\n",
            "          [ 5.4391e-02,  4.8974e-02,  7.6901e-03],\n",
            "          [ 3.2155e-03,  2.4854e-02,  4.5191e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7717e-02, -1.6405e-02,  3.3350e-02],\n",
            "          [-4.0021e-02,  7.3493e-03,  1.5939e-02],\n",
            "          [-4.0612e-04, -6.6383e-04, -5.2255e-03]],\n",
            "\n",
            "         [[ 4.7864e-02,  2.8927e-02, -1.1633e-03],\n",
            "          [-2.9743e-02, -3.7254e-02, -3.5107e-02],\n",
            "          [-1.8876e-02,  2.1955e-02,  3.9075e-02]],\n",
            "\n",
            "         [[-1.9286e-02, -7.1456e-03,  3.6255e-02],\n",
            "          [-4.2145e-02,  2.2160e-02,  7.0113e-03],\n",
            "          [-7.9427e-02, -6.1677e-02, -6.2344e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.0205e-02, -2.3498e-02,  1.4196e-02],\n",
            "          [ 5.3369e-03, -7.4289e-05, -6.5851e-03],\n",
            "          [ 2.8512e-02,  7.8920e-02,  1.8009e-02]],\n",
            "\n",
            "         [[-1.0774e-01, -1.1033e-01, -7.0797e-02],\n",
            "          [ 1.9243e-02, -8.1254e-03,  1.7706e-02],\n",
            "          [ 1.1645e-01,  1.5353e-01,  1.0827e-01]],\n",
            "\n",
            "         [[-2.5730e-02,  8.3504e-03,  8.4165e-03],\n",
            "          [-1.3629e-03, -1.6058e-02, -1.9505e-02],\n",
            "          [-1.0030e-01, -9.3626e-02, -1.0695e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2889e-02, -1.5862e-02, -2.5936e-02],\n",
            "          [-5.4724e-02,  1.1983e-03, -4.3311e-02],\n",
            "          [ 1.5783e-02, -8.5597e-03, -2.8347e-02]],\n",
            "\n",
            "         [[ 2.3205e-02, -1.8653e-03, -2.6231e-02],\n",
            "          [ 4.2133e-02,  1.5103e-02,  8.6894e-03],\n",
            "          [ 8.0215e-03,  4.0410e-02,  3.9687e-02]],\n",
            "\n",
            "         [[-1.3756e-02, -5.1843e-02, -1.7923e-02],\n",
            "          [-1.4185e-02,  6.0515e-03,  3.8190e-02],\n",
            "          [-1.6701e-02, -1.0463e-02, -2.8779e-03]]]])\n",
            "conv2.bias: tensor([-0.0314, -0.0315,  0.0054, -0.0144,  0.0025, -0.0306,  0.0144,  0.0006,\n",
            "         0.0273,  0.0113,  0.0139,  0.0201, -0.0279,  0.0360, -0.0247,  0.0329,\n",
            "        -0.0004, -0.0247,  0.0297, -0.0165, -0.0163, -0.0217,  0.0258, -0.0404,\n",
            "         0.0307,  0.0234,  0.0290,  0.0210, -0.0219, -0.0043, -0.0275,  0.0110,\n",
            "        -0.0158,  0.0004,  0.0068,  0.0013,  0.0113,  0.0324, -0.0370,  0.0303,\n",
            "         0.0054, -0.0383, -0.0045,  0.0047,  0.0179, -0.0380,  0.0280, -0.0335,\n",
            "        -0.0029,  0.0344, -0.0112, -0.0078, -0.0369, -0.0180,  0.0332,  0.0094,\n",
            "        -0.0089,  0.0178, -0.0377,  0.0391, -0.0139,  0.0263, -0.0286,  0.0317,\n",
            "         0.0160, -0.0406,  0.0290, -0.0090,  0.0224, -0.0347,  0.0255,  0.0079,\n",
            "        -0.0011,  0.0186, -0.0011, -0.0311,  0.0011,  0.0190,  0.0302, -0.0018,\n",
            "        -0.0022, -0.0004,  0.0139, -0.0224, -0.0037,  0.0099,  0.0073,  0.0339,\n",
            "         0.0128, -0.0304, -0.0033, -0.0384, -0.0016, -0.0051,  0.0220, -0.0266,\n",
            "         0.0215,  0.0208, -0.0279,  0.0014,  0.0374,  0.0313,  0.0303, -0.0042,\n",
            "         0.0318, -0.0130, -0.0285, -0.0325, -0.0311, -0.0346, -0.0339, -0.0396,\n",
            "         0.0338,  0.0154,  0.0410,  0.0331,  0.0095, -0.0236, -0.0206, -0.0084,\n",
            "         0.0309,  0.0128,  0.0311,  0.0236, -0.0360, -0.0002,  0.0344,  0.0217])\n",
            "bn2.weight: tensor([0.9961, 0.9156, 0.9576, 1.1241, 0.9157, 0.9780, 1.0616, 0.9431, 0.9421,\n",
            "        0.9816, 0.9684, 0.9872, 1.0609, 0.9551, 1.0597, 0.9606, 0.9929, 1.0639,\n",
            "        1.0327, 0.9052, 0.9624, 0.9382, 0.9768, 0.9751, 1.0156, 1.1647, 1.1107,\n",
            "        0.9738, 0.9972, 0.9275, 0.9307, 0.9816, 0.9916, 0.9626, 1.0211, 0.9660,\n",
            "        0.8629, 0.9317, 0.9121, 0.9290, 1.0717, 0.9948, 1.0659, 1.0010, 1.0986,\n",
            "        1.0352, 0.8843, 0.9496, 0.9378, 1.0615, 1.0151, 1.0275, 1.0359, 1.0032,\n",
            "        1.0272, 0.9328, 1.0671, 0.9887, 0.9435, 0.8879, 0.8505, 1.0246, 0.9715,\n",
            "        0.9937, 0.9276, 0.9949, 1.0076, 0.9939, 0.9231, 1.0598, 1.0124, 0.9551,\n",
            "        1.0209, 0.9175, 0.9885, 1.0788, 0.9322, 0.9997, 0.9396, 0.9212, 0.9005,\n",
            "        1.0320, 1.0189, 1.0829, 0.9325, 0.9936, 1.0190, 0.9467, 0.9748, 1.0793,\n",
            "        0.9411, 0.9397, 0.8766, 1.0537, 1.0179, 0.8959, 1.0855, 1.0374, 1.1738,\n",
            "        0.9822, 0.9690, 0.9652, 0.9512, 0.9653, 0.9746, 0.9156, 0.9687, 0.9322,\n",
            "        0.9333, 0.9872, 0.9413, 0.9203, 0.9417, 1.0114, 0.9152, 0.9305, 1.0342,\n",
            "        1.1019, 0.9528, 0.9967, 1.1094, 1.0532, 0.9732, 1.0247, 0.9756, 0.9913,\n",
            "        1.0027, 1.0393])\n",
            "bn2.bias: tensor([-0.1794, -0.1333, -0.2359, -0.1723, -0.1679, -0.1793, -0.1519, -0.1494,\n",
            "        -0.0782, -0.1324, -0.1244, -0.1547, -0.1967, -0.1874, -0.1933, -0.1839,\n",
            "        -0.1414, -0.1416, -0.1322, -0.1586, -0.1153, -0.1355, -0.0948, -0.1850,\n",
            "        -0.0923, -0.1460, -0.3156, -0.1240, -0.1688, -0.1144, -0.0554, -0.1392,\n",
            "        -0.1682, -0.1467, -0.1147, -0.1211, -0.1323, -0.1945, -0.1627, -0.1321,\n",
            "        -0.1357, -0.1142, -0.1492, -0.1753, -0.2051, -0.2068, -0.1456, -0.1418,\n",
            "        -0.1245, -0.2265, -0.1746, -0.0876, -0.1827, -0.0907, -0.1550, -0.1087,\n",
            "        -0.1339, -0.1283, -0.1399, -0.1159, -0.1543, -0.0917, -0.1170, -0.1197,\n",
            "        -0.1657, -0.2258, -0.2103, -0.1598, -0.1924, -0.1729, -0.0984, -0.1190,\n",
            "        -0.1366, -0.2502, -0.1922, -0.2232, -0.0861, -0.1051, -0.2437, -0.1065,\n",
            "        -0.1268, -0.1396, -0.0622, -0.2066, -0.1134, -0.1583, -0.2288, -0.2120,\n",
            "        -0.0663, -0.1958, -0.1572, -0.1881, -0.1700, -0.1102, -0.1099, -0.1272,\n",
            "        -0.2194, -0.1109, -0.3141, -0.1724, -0.1634, -0.1926, -0.2054, -0.1437,\n",
            "        -0.1533, -0.1546, -0.0828, -0.1481, -0.1187, -0.1011, -0.1909, -0.1156,\n",
            "        -0.1226, -0.1037, -0.1357, -0.1237, -0.1605, -0.1119, -0.1557, -0.1114,\n",
            "        -0.2158, -0.1949, -0.1032, -0.0487, -0.1719, -0.1786, -0.0835, -0.1754])\n",
            "bn2.running_mean: tensor([-1.6755, -0.2631, -0.2376, -3.2859,  0.1957, -0.0459, -1.6956,  2.8071,\n",
            "        -0.6945, -1.6642, -1.4862, -2.0517, -1.9615, -0.2696, -1.2532,  2.3292,\n",
            "        -0.9581, -1.6026, -1.4472, -0.1977, -1.7393, -1.1659, -0.8412,  0.5237,\n",
            "        -1.3400, -1.5667, -0.5421, -0.9378, -1.4823, -0.6869, -0.8138, -1.0221,\n",
            "        -1.4184, -2.0645, -1.5485, -0.8360,  0.2905, -1.9371,  0.1775,  0.2943,\n",
            "        -1.4786, -2.4530, -1.3507, -1.4578, -1.0160, -0.8452, -0.9854, -1.7888,\n",
            "        -1.7168, -0.4222, -1.4659, -1.3887, -2.2804, -0.7531, -2.6335, -0.9460,\n",
            "        -1.3756, -1.3911, -1.2509, -1.0273,  0.0663, -1.6794, -0.4855, -1.0163,\n",
            "        -0.5358, -0.5589, -1.5869, -1.2247, -0.0315, -1.4731, -1.0543, -0.2111,\n",
            "        -1.4015, -1.3673, -0.8815, -2.2594, -0.9582, -1.1557, -1.1500,  1.2224,\n",
            "        -0.0583, -2.5527, -0.9392, -2.8373, -1.7442, -0.0613,  0.3361, -0.5101,\n",
            "        -0.6625, -2.5869, -1.0592,  0.0974,  1.1349, -2.6810, -1.3332, -0.6359,\n",
            "        -1.2574, -2.1877, -0.8033, -0.7227, -1.4529, -1.8894, -0.5731, -1.1100,\n",
            "        -1.0626, -0.5056, -1.2162, -1.1085, -1.7542, -1.5746, -1.1850,  0.0116,\n",
            "        -1.0980, -1.7401, -0.5043, -1.2922, -1.5494, -0.9473, -1.2592, -0.6532,\n",
            "        -2.8069, -1.9882, -1.3970,  0.1161, -1.4786, -0.8511, -2.1660, -1.3029])\n",
            "bn2.running_var: tensor([1.4390, 1.4467, 1.4678, 4.5106, 1.4801, 1.8614, 2.1943, 2.9427, 1.5499,\n",
            "        1.7670, 1.1494, 2.3403, 2.3000, 1.8168, 2.0992, 2.1788, 1.5291, 2.4200,\n",
            "        2.2898, 1.7622, 1.7625, 2.0035, 1.3019, 1.8147, 1.7377, 2.1699, 3.5820,\n",
            "        2.0908, 2.3202, 1.4790, 1.7986, 1.3663, 1.4132, 1.8706, 1.7935, 1.1766,\n",
            "        1.4746, 1.9468, 1.9348, 1.3760, 1.8214, 2.4659, 1.9638, 1.2189, 2.2789,\n",
            "        2.1498, 1.7537, 1.5884, 1.9796, 1.3340, 1.5623, 1.8876, 2.1680, 1.8265,\n",
            "        3.1076, 0.9658, 1.8490, 1.3284, 2.7681, 1.5133, 1.3934, 2.0373, 1.2241,\n",
            "        1.2270, 1.3149, 1.2934, 2.2377, 1.6638, 1.5229, 2.0419, 1.2506, 1.7858,\n",
            "        1.8241, 1.9623, 2.1823, 3.1333, 1.6880, 1.3752, 2.6168, 1.8222, 1.6565,\n",
            "        3.2754, 1.6221, 2.9352, 1.1712, 2.2421, 3.0716, 1.3143, 1.1634, 3.3579,\n",
            "        1.1602, 1.6207, 2.4689, 2.7192, 1.4938, 1.2698, 2.0532, 2.9316, 3.8433,\n",
            "        1.9855, 1.9031, 1.4301, 1.8799, 1.8876, 1.8918, 1.1459, 1.4242, 1.5629,\n",
            "        1.6765, 1.4699, 1.9561, 1.6169, 1.4697, 2.1655, 1.7872, 1.6235, 1.9819,\n",
            "        1.7025, 1.4339, 1.0480, 5.3233, 3.6034, 1.4320, 2.8665, 1.5448, 1.8237,\n",
            "        1.8821, 1.6469])\n",
            "bn2.num_batches_tracked: 5100\n",
            "conv3.weight: tensor([[[[ 1.5454e-02, -4.7840e-02, -7.4507e-03],\n",
            "          [-3.9090e-02, -1.5147e-02,  1.1323e-02],\n",
            "          [-3.6398e-03,  1.7070e-02, -1.1929e-02]],\n",
            "\n",
            "         [[ 1.5864e-02, -1.4634e-02,  2.4032e-02],\n",
            "          [-9.4033e-03,  3.8208e-03,  5.5331e-03],\n",
            "          [-4.7673e-03,  1.2032e-03,  2.9118e-02]],\n",
            "\n",
            "         [[-1.3337e-03,  6.9978e-03,  1.9790e-02],\n",
            "          [ 2.7266e-02, -2.5598e-02,  1.7734e-02],\n",
            "          [-2.9448e-02,  2.0676e-02,  6.2755e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6676e-02, -7.0444e-03, -1.0575e-03],\n",
            "          [ 2.6302e-02, -1.3716e-02,  1.3011e-02],\n",
            "          [ 2.0072e-03,  1.7410e-02, -1.4889e-02]],\n",
            "\n",
            "         [[ 2.2288e-02,  2.5913e-02, -3.9731e-02],\n",
            "          [ 2.2893e-02,  5.6227e-02, -3.1806e-02],\n",
            "          [ 5.9697e-03,  7.0090e-03, -3.1337e-02]],\n",
            "\n",
            "         [[ 1.0753e-02, -4.3030e-02,  1.1620e-06],\n",
            "          [ 2.7617e-02, -2.6116e-02, -2.7685e-02],\n",
            "          [ 7.2505e-03, -3.1891e-02, -3.2626e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.1138e-02,  2.6704e-02, -8.7580e-05],\n",
            "          [-4.8859e-02,  4.0186e-02,  2.7459e-02],\n",
            "          [-2.0038e-02, -3.5901e-03,  3.2784e-02]],\n",
            "\n",
            "         [[-1.9782e-04,  7.6954e-03, -7.5272e-03],\n",
            "          [-2.7422e-02, -3.5720e-02, -5.2586e-03],\n",
            "          [ 6.1144e-03,  9.5296e-03,  8.5040e-03]],\n",
            "\n",
            "         [[-6.3051e-03,  1.4487e-02,  9.5297e-03],\n",
            "          [-5.1158e-03,  2.3746e-02,  2.9239e-02],\n",
            "          [-2.1045e-03, -9.6766e-03, -1.7054e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2284e-03, -4.9036e-03,  1.6170e-02],\n",
            "          [-4.5639e-03, -3.6166e-03, -7.3351e-04],\n",
            "          [ 8.9542e-03, -3.3292e-02, -2.6032e-03]],\n",
            "\n",
            "         [[-3.3019e-03, -3.9585e-03, -2.4152e-02],\n",
            "          [ 2.7523e-02, -3.1273e-02, -5.4480e-03],\n",
            "          [ 1.1240e-02,  2.0813e-02,  3.5717e-03]],\n",
            "\n",
            "         [[-2.0678e-02, -5.0672e-03,  6.1664e-03],\n",
            "          [-1.1724e-02, -2.7927e-02,  5.6961e-03],\n",
            "          [-2.9880e-02,  4.4371e-03,  5.8568e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 6.7868e-04,  2.5943e-02, -2.4206e-02],\n",
            "          [-3.0684e-02, -1.0200e-02, -1.3352e-02],\n",
            "          [-3.3243e-02, -3.0669e-02,  1.6020e-03]],\n",
            "\n",
            "         [[ 3.6639e-02, -2.6088e-03,  1.6924e-02],\n",
            "          [ 1.5537e-02, -2.9867e-02,  2.6844e-03],\n",
            "          [-2.3805e-02,  1.1350e-03,  1.3671e-02]],\n",
            "\n",
            "         [[-5.2798e-03,  2.1790e-02, -1.1752e-02],\n",
            "          [-1.6626e-02, -2.2123e-02,  1.6287e-04],\n",
            "          [-3.8606e-03, -3.9363e-02, -1.3732e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.1208e-03, -1.0843e-02, -2.4159e-02],\n",
            "          [-4.3322e-04,  5.5138e-03,  1.3633e-02],\n",
            "          [-1.3619e-03,  1.0200e-02, -2.2560e-02]],\n",
            "\n",
            "         [[ 7.3147e-03, -3.0083e-02,  2.1819e-02],\n",
            "          [ 1.7146e-02,  1.1580e-02,  5.8488e-03],\n",
            "          [-1.3799e-02,  4.4708e-04, -9.8666e-03]],\n",
            "\n",
            "         [[ 1.5935e-02,  1.4078e-02,  2.2661e-02],\n",
            "          [ 3.9770e-02,  4.0989e-02,  3.9468e-02],\n",
            "          [ 3.2682e-02,  4.5031e-02,  1.3788e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.6003e-02, -2.5434e-03, -6.8508e-03],\n",
            "          [ 3.7972e-02,  2.1143e-02,  1.2294e-02],\n",
            "          [ 3.1144e-02,  1.5521e-02, -2.2925e-02]],\n",
            "\n",
            "         [[ 1.8226e-02, -1.6666e-02,  3.4423e-02],\n",
            "          [-1.6716e-02, -2.5484e-02, -1.5883e-02],\n",
            "          [ 1.0656e-02,  7.9681e-03,  2.1462e-02]],\n",
            "\n",
            "         [[-1.1453e-02,  8.5563e-03,  5.8757e-03],\n",
            "          [-1.7744e-02, -4.4873e-03, -6.6149e-03],\n",
            "          [ 7.4677e-04,  1.1847e-02,  6.3630e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0018e-02,  1.1254e-02, -1.0223e-02],\n",
            "          [ 3.5337e-02,  4.3858e-03, -2.0524e-02],\n",
            "          [-3.6989e-04,  2.4111e-02, -3.9781e-02]],\n",
            "\n",
            "         [[-1.2539e-02, -9.4658e-03,  1.2675e-02],\n",
            "          [-6.6334e-03,  1.1674e-02,  2.4229e-02],\n",
            "          [ 7.8714e-03, -1.6957e-02,  1.0845e-02]],\n",
            "\n",
            "         [[ 2.6343e-02,  1.8291e-03,  2.4229e-02],\n",
            "          [-1.3849e-02, -3.4519e-03, -6.2020e-04],\n",
            "          [ 3.2173e-03, -3.0518e-02, -3.2259e-02]]],\n",
            "\n",
            "\n",
            "        [[[-6.5527e-03, -1.4304e-02, -1.4175e-02],\n",
            "          [ 1.0516e-02,  3.3765e-02,  3.0490e-03],\n",
            "          [-3.0794e-02,  1.1454e-02,  2.0599e-02]],\n",
            "\n",
            "         [[-8.7486e-03, -5.1234e-03,  2.4214e-02],\n",
            "          [-1.5887e-02, -7.2606e-03, -1.5424e-03],\n",
            "          [ 5.9788e-03,  6.9252e-03,  1.3554e-02]],\n",
            "\n",
            "         [[ 9.1067e-03,  1.4140e-02,  1.6657e-02],\n",
            "          [-2.7136e-02, -3.4461e-02, -8.0477e-03],\n",
            "          [-1.0557e-02, -9.6284e-03,  1.7018e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.1610e-02,  1.9837e-02,  2.0895e-02],\n",
            "          [ 1.9931e-02,  2.3021e-02, -1.2416e-02],\n",
            "          [ 1.5960e-02, -1.9190e-02, -5.2756e-03]],\n",
            "\n",
            "         [[ 3.1193e-02, -1.9346e-02,  3.0284e-03],\n",
            "          [ 4.3195e-03,  1.3865e-02, -2.0477e-02],\n",
            "          [ 4.6717e-03, -2.1775e-02,  7.2630e-03]],\n",
            "\n",
            "         [[ 1.9353e-02, -1.5926e-02, -9.2710e-03],\n",
            "          [ 8.7186e-03,  1.0316e-03,  1.1109e-02],\n",
            "          [-3.4843e-03, -9.0521e-03, -3.7744e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6134e-02, -2.4547e-02,  8.1155e-03],\n",
            "          [ 2.0440e-02, -2.2708e-02,  4.7863e-03],\n",
            "          [ 3.0199e-03,  1.9758e-02,  1.5836e-02]],\n",
            "\n",
            "         [[-6.8383e-03, -3.6529e-02,  1.6275e-02],\n",
            "          [-3.9723e-03, -5.0169e-02, -1.1266e-02],\n",
            "          [-7.1642e-03, -2.6907e-03,  4.5527e-03]],\n",
            "\n",
            "         [[ 1.5249e-02, -1.8749e-02,  1.5318e-02],\n",
            "          [ 4.2624e-02,  3.3101e-02,  2.4634e-02],\n",
            "          [ 3.4689e-02,  8.5061e-03, -3.6524e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.7818e-02, -2.2231e-02, -2.7582e-02],\n",
            "          [-1.7514e-03,  8.1046e-03, -1.3361e-02],\n",
            "          [ 1.9530e-02, -2.3618e-02,  1.9338e-02]],\n",
            "\n",
            "         [[-4.0911e-03, -6.3737e-03,  5.2289e-03],\n",
            "          [-2.5082e-03,  3.9343e-02,  2.8835e-02],\n",
            "          [-1.0883e-02, -1.6624e-02,  2.6649e-02]],\n",
            "\n",
            "         [[-2.5376e-02, -3.8879e-02, -9.3083e-04],\n",
            "          [ 9.5638e-03,  1.9319e-03, -9.7454e-03],\n",
            "          [ 1.6215e-02,  2.8096e-02, -1.1167e-02]]]])\n",
            "conv3.bias: tensor([-0.0050, -0.0281, -0.0074,  0.0180,  0.0108, -0.0291, -0.0099, -0.0223,\n",
            "        -0.0122, -0.0243,  0.0090, -0.0021,  0.0147,  0.0038, -0.0282,  0.0028,\n",
            "         0.0115,  0.0094,  0.0069, -0.0125,  0.0095,  0.0059, -0.0037, -0.0117,\n",
            "         0.0172,  0.0173, -0.0223,  0.0032,  0.0277, -0.0136,  0.0144, -0.0152,\n",
            "         0.0275,  0.0022,  0.0034, -0.0276, -0.0113, -0.0024, -0.0160, -0.0067,\n",
            "         0.0142, -0.0268, -0.0173,  0.0079,  0.0002, -0.0079, -0.0146, -0.0084,\n",
            "         0.0249, -0.0089, -0.0255, -0.0270, -0.0266, -0.0153, -0.0106, -0.0074,\n",
            "        -0.0162,  0.0122, -0.0009, -0.0185, -0.0111, -0.0274,  0.0019,  0.0036,\n",
            "         0.0197, -0.0286,  0.0049, -0.0293, -0.0127, -0.0236,  0.0017,  0.0213,\n",
            "        -0.0145,  0.0233, -0.0173,  0.0079,  0.0074,  0.0172,  0.0262, -0.0242,\n",
            "         0.0196, -0.0191, -0.0164, -0.0146,  0.0052, -0.0104, -0.0283,  0.0229,\n",
            "        -0.0023, -0.0110, -0.0158,  0.0250, -0.0241,  0.0197, -0.0192,  0.0010,\n",
            "        -0.0234, -0.0018, -0.0190, -0.0077, -0.0042, -0.0242, -0.0006,  0.0277,\n",
            "         0.0067,  0.0277, -0.0262,  0.0282, -0.0246,  0.0091, -0.0046,  0.0276,\n",
            "         0.0111,  0.0217,  0.0103, -0.0159, -0.0205, -0.0008,  0.0019,  0.0196,\n",
            "        -0.0271, -0.0240,  0.0221,  0.0022,  0.0139, -0.0091, -0.0043,  0.0094,\n",
            "        -0.0008,  0.0286,  0.0162, -0.0132, -0.0112, -0.0115,  0.0134, -0.0249,\n",
            "         0.0154,  0.0005, -0.0085, -0.0266, -0.0220, -0.0067,  0.0048,  0.0181,\n",
            "         0.0092,  0.0105, -0.0042,  0.0161,  0.0143,  0.0059, -0.0016, -0.0115,\n",
            "         0.0198, -0.0059,  0.0080,  0.0090, -0.0051, -0.0074,  0.0239, -0.0195,\n",
            "        -0.0246,  0.0006,  0.0060,  0.0073,  0.0112,  0.0073, -0.0282, -0.0058,\n",
            "         0.0118, -0.0170, -0.0076,  0.0198,  0.0272, -0.0244,  0.0213, -0.0268,\n",
            "         0.0121,  0.0022,  0.0084,  0.0217,  0.0039, -0.0057,  0.0258, -0.0060,\n",
            "        -0.0228, -0.0144,  0.0154, -0.0200,  0.0188, -0.0284,  0.0170,  0.0222,\n",
            "         0.0217, -0.0271, -0.0245,  0.0185, -0.0176, -0.0116,  0.0142, -0.0006,\n",
            "        -0.0195, -0.0058, -0.0143, -0.0113, -0.0015,  0.0036, -0.0011,  0.0203,\n",
            "        -0.0279,  0.0239,  0.0028, -0.0213, -0.0185, -0.0292, -0.0015, -0.0125,\n",
            "         0.0275, -0.0079,  0.0215, -0.0217, -0.0057,  0.0050, -0.0173,  0.0175,\n",
            "         0.0273,  0.0036,  0.0086, -0.0066, -0.0204,  0.0087,  0.0053, -0.0155,\n",
            "        -0.0056,  0.0117,  0.0092,  0.0264,  0.0100, -0.0172, -0.0010, -0.0058,\n",
            "        -0.0125, -0.0084,  0.0066,  0.0064, -0.0046, -0.0121,  0.0289, -0.0134,\n",
            "        -0.0205, -0.0163,  0.0220,  0.0161, -0.0171, -0.0112, -0.0160, -0.0155])\n",
            "bn3.weight: tensor([0.8994, 1.0166, 0.9062, 0.9381, 0.9849, 0.9029, 0.9351, 0.8980, 0.9646,\n",
            "        1.0037, 0.8976, 0.9219, 0.8829, 0.9520, 1.1228, 0.9646, 0.9670, 0.9462,\n",
            "        1.0470, 0.9530, 0.9341, 0.8866, 1.0586, 0.9595, 0.9275, 0.9214, 0.9554,\n",
            "        0.9958, 0.9259, 0.9309, 0.9315, 1.0506, 0.9871, 0.9102, 0.9865, 0.9199,\n",
            "        0.8760, 1.0126, 1.0061, 0.9332, 0.9155, 1.0769, 0.9464, 0.9888, 0.9179,\n",
            "        1.0110, 0.9857, 0.9401, 0.9605, 1.1246, 0.9415, 0.9061, 0.9898, 0.9040,\n",
            "        1.0471, 0.8971, 0.9432, 0.9966, 0.9164, 1.0002, 1.0088, 0.9350, 0.9244,\n",
            "        0.9417, 0.9764, 0.9720, 0.9281, 0.9261, 0.9269, 0.9877, 0.8989, 1.0298,\n",
            "        0.9157, 0.9798, 0.9898, 0.9458, 0.9208, 0.9630, 0.9821, 0.9262, 0.9242,\n",
            "        0.9608, 1.0582, 0.9617, 0.9456, 1.0302, 0.9679, 0.9200, 0.9787, 0.9166,\n",
            "        0.9183, 0.9132, 0.9282, 0.9628, 0.9665, 0.9348, 0.9609, 0.8747, 0.9949,\n",
            "        0.9131, 1.0009, 0.9439, 0.9475, 0.9589, 0.9390, 0.9671, 0.9883, 0.8958,\n",
            "        0.9055, 0.9348, 0.9861, 0.9213, 0.9199, 0.9971, 0.9394, 0.9382, 0.9484,\n",
            "        0.9640, 0.9381, 0.9138, 0.9653, 0.9170, 0.9285, 0.8979, 1.0228, 0.9345,\n",
            "        0.9195, 1.0032, 0.9685, 0.9470, 0.9432, 0.9325, 0.9195, 0.9231, 1.0157,\n",
            "        0.9164, 0.9413, 0.8974, 1.0266, 0.9126, 0.8890, 0.9526, 0.9597, 0.9197,\n",
            "        0.9380, 0.9755, 1.0167, 1.0105, 0.9846, 0.9690, 1.0041, 0.9705, 0.9492,\n",
            "        0.9072, 0.9424, 0.9485, 0.9531, 0.9104, 0.9233, 0.9620, 0.9283, 0.9408,\n",
            "        0.9085, 0.8973, 0.9353, 1.0153, 0.9332, 0.9431, 0.9326, 0.9466, 0.9408,\n",
            "        0.8971, 0.9656, 0.9752, 0.9447, 1.0156, 1.1279, 0.9386, 0.9007, 0.8910,\n",
            "        0.9229, 1.0414, 0.8985, 0.9558, 0.9078, 0.9166, 0.9327, 0.9320, 0.9058,\n",
            "        0.9263, 0.9346, 1.0081, 0.9019, 0.9041, 0.8979, 0.9777, 0.9647, 0.9469,\n",
            "        0.9669, 0.9404, 1.0078, 0.9565, 0.9874, 1.0212, 0.8891, 0.9455, 0.9574,\n",
            "        0.9028, 0.9183, 0.9203, 0.9294, 0.9111, 0.9109, 0.9198, 0.9665, 0.9467,\n",
            "        0.9431, 0.9085, 0.9552, 0.9001, 0.9694, 0.9158, 0.8804, 0.9605, 0.9472,\n",
            "        0.8939, 0.9076, 0.9419, 0.9789, 1.0124, 1.0360, 0.9911, 0.9884, 0.9286,\n",
            "        0.9565, 1.0015, 0.9817, 0.9085, 0.9418, 0.9188, 0.9518, 0.9634, 1.0468,\n",
            "        0.8864, 0.9332, 0.9082, 1.0013, 0.9488, 1.0084, 0.9427, 0.9191, 0.9069,\n",
            "        0.9113, 0.9832, 0.9040, 0.9155])\n",
            "bn3.bias: tensor([-0.1977, -0.1821, -0.1839, -0.1974, -0.2361, -0.1877, -0.1866, -0.1860,\n",
            "        -0.2114, -0.2246, -0.1814, -0.1623, -0.1682, -0.2027, -0.2243, -0.2204,\n",
            "        -0.1991, -0.1892, -0.2078, -0.2047, -0.2172, -0.1801, -0.2322, -0.1798,\n",
            "        -0.2077, -0.2064, -0.2485, -0.2234, -0.1866, -0.1991, -0.1990, -0.2889,\n",
            "        -0.2222, -0.1499, -0.2477, -0.2013, -0.1666, -0.2458, -0.2483, -0.2421,\n",
            "        -0.1797, -0.2888, -0.1933, -0.2327, -0.1946, -0.1957, -0.2427, -0.2205,\n",
            "        -0.1946, -0.3064, -0.1808, -0.1774, -0.1714, -0.1648, -0.2322, -0.1669,\n",
            "        -0.2418, -0.2462, -0.2199, -0.2085, -0.2500, -0.2014, -0.2151, -0.2005,\n",
            "        -0.1911, -0.1958, -0.2001, -0.1936, -0.1874, -0.2212, -0.1796, -0.2077,\n",
            "        -0.1899, -0.1905, -0.2059, -0.1745, -0.1624, -0.2154, -0.2412, -0.1939,\n",
            "        -0.2159, -0.2128, -0.2239, -0.2039, -0.2044, -0.2604, -0.1853, -0.2056,\n",
            "        -0.1866, -0.2160, -0.2163, -0.1727, -0.2018, -0.2022, -0.2005, -0.1868,\n",
            "        -0.1893, -0.1738, -0.1745, -0.1769, -0.2482, -0.2346, -0.1973, -0.1906,\n",
            "        -0.1970, -0.2295, -0.2321, -0.1792, -0.1725, -0.2015, -0.2102, -0.1807,\n",
            "        -0.1803, -0.2030, -0.2190, -0.1965, -0.1735, -0.2097, -0.1876, -0.1718,\n",
            "        -0.2066, -0.2001, -0.1758, -0.1809, -0.2224, -0.1463, -0.2086, -0.2447,\n",
            "        -0.1830, -0.2328, -0.1853, -0.2321, -0.1579, -0.2237, -0.2137, -0.1632,\n",
            "        -0.2184, -0.1563, -0.2090, -0.2199, -0.2112, -0.1539, -0.1982, -0.1786,\n",
            "        -0.2000, -0.2336, -0.2443, -0.1972, -0.2301, -0.1984, -0.1896, -0.2080,\n",
            "        -0.1924, -0.1866, -0.1581, -0.1777, -0.2387, -0.1818, -0.1979, -0.1908,\n",
            "        -0.1807, -0.2226, -0.2107, -0.1960, -0.2164, -0.2316, -0.1785, -0.1823,\n",
            "        -0.1938, -0.1851, -0.2192, -0.2103, -0.1531, -0.2174, -0.2232, -0.2076,\n",
            "        -0.3145, -0.1715, -0.2048, -0.1695, -0.2057, -0.2420, -0.1717, -0.2071,\n",
            "        -0.1965, -0.2002, -0.2078, -0.1781, -0.1793, -0.2039, -0.2168, -0.2655,\n",
            "        -0.1821, -0.1790, -0.1772, -0.2053, -0.2038, -0.2130, -0.1856, -0.2003,\n",
            "        -0.1858, -0.1907, -0.2624, -0.1739, -0.1956, -0.2395, -0.2118, -0.1882,\n",
            "        -0.2236, -0.1901, -0.2023, -0.1728, -0.1901, -0.1689, -0.1824, -0.1856,\n",
            "        -0.2228, -0.1872, -0.2097, -0.1715, -0.2576, -0.1818, -0.1827, -0.2003,\n",
            "        -0.2333, -0.1745, -0.1915, -0.1776, -0.2041, -0.2627, -0.2135, -0.2582,\n",
            "        -0.2014, -0.1893, -0.1949, -0.2322, -0.1928, -0.1795, -0.1868, -0.2053,\n",
            "        -0.2004, -0.2345, -0.2988, -0.1998, -0.2269, -0.2173, -0.2110, -0.1778,\n",
            "        -0.2178, -0.2096, -0.2163, -0.1974, -0.1779, -0.2282, -0.1542, -0.1903])\n",
            "bn3.running_mean: tensor([ 1.6905e-01, -6.7168e-01, -8.6295e-01, -6.7521e-01,  2.7263e-01,\n",
            "        -9.6641e-01, -1.2483e-01, -1.3872e+00, -6.9120e-01, -1.1700e+00,\n",
            "        -8.0216e-02, -4.4691e-01, -9.0484e-01, -3.4825e-01, -3.9686e-01,\n",
            "        -9.7038e-01, -9.4068e-01, -8.6708e-01, -4.9775e-01, -1.0170e+00,\n",
            "         5.2313e-02, -1.1556e+00,  9.6986e-01,  5.1011e-01, -4.5602e-01,\n",
            "        -9.4468e-01,  4.4541e-01,  5.1041e-01, -1.4142e+00, -1.2585e+00,\n",
            "         4.0964e-01, -4.2495e-01,  5.6846e-01, -4.9599e-02, -4.3300e-01,\n",
            "        -5.7385e-01, -3.0622e-01, -4.3539e-01, -4.9375e-01,  2.4407e-01,\n",
            "        -1.1073e+00, -9.1932e-01, -2.6624e-01, -1.7625e-01, -7.5189e-01,\n",
            "        -9.9498e-01, -1.3613e+00,  4.9106e-01, -5.4610e-01, -4.4444e-02,\n",
            "        -1.0603e+00, -1.1377e+00, -4.9200e-01, -3.7336e-01,  7.3526e-01,\n",
            "        -7.3993e-01,  6.3022e-01, -1.8486e-02,  8.3545e-01, -1.2135e+00,\n",
            "        -8.4700e-02,  5.2903e-02, -5.3784e-01, -1.9468e-01, -1.3807e+00,\n",
            "        -1.5363e+00, -6.1394e-02,  1.1344e-01, -1.7556e+00, -1.5359e+00,\n",
            "        -8.1609e-01, -1.2336e+00, -7.4185e-01, -1.3005e+00, -1.5440e+00,\n",
            "        -3.4543e-01,  2.5080e-01,  6.5079e-01,  1.7140e-01, -1.0231e+00,\n",
            "        -1.9553e-01, -5.6682e-01, -5.5134e-01, -6.1547e-01, -2.5768e+00,\n",
            "        -1.2418e+00, -2.3214e+00,  1.2828e+00, -8.9980e-01, -4.1791e-01,\n",
            "         1.3170e-01, -1.2872e+00,  5.5289e-02, -1.2949e+00,  1.6058e+00,\n",
            "        -1.7707e+00, -1.3275e+00, -1.2929e+00, -8.9276e-01, -4.5963e-01,\n",
            "         2.2758e-01, -8.4181e-01, -1.4824e+00, -2.1050e+00, -1.6849e+00,\n",
            "         1.4793e+00,  6.8461e-01, -1.8474e+00, -1.7620e+00, -2.4715e-01,\n",
            "         3.7615e-01, -6.0950e-01, -7.5258e-01, -1.5546e+00,  6.8906e-01,\n",
            "        -1.9919e+00,  3.7614e-01, -9.9493e-01, -7.8253e-02, -7.1545e-01,\n",
            "        -8.0481e-01, -1.3447e+00, -1.0077e+00, -2.9030e-02, -1.4680e+00,\n",
            "        -7.8477e-01, -1.1931e-01,  3.0189e-01, -3.3279e-01,  6.7214e-01,\n",
            "        -6.6994e-01, -8.4236e-01, -1.8924e+00, -4.1093e-01, -2.0724e-01,\n",
            "        -1.6313e+00, -8.7262e-01, -1.4455e-02,  4.0796e-01,  4.6370e-01,\n",
            "        -1.9076e+00, -8.9626e-01, -2.0842e+00, -1.6670e+00, -6.0708e-01,\n",
            "        -2.9344e-01,  1.8136e-01, -1.3496e+00, -6.7348e-01, -2.5394e-01,\n",
            "         2.3063e-01, -1.1520e+00, -6.4095e-01, -3.1409e-01, -1.2105e+00,\n",
            "        -1.2206e+00,  7.5471e-01, -1.4717e+00, -1.0004e-01, -5.0650e-01,\n",
            "        -1.8440e+00, -8.1917e-01, -4.3722e-01, -8.7194e-01, -3.7059e-01,\n",
            "         8.8765e-01, -1.9210e+00, -2.2249e+00, -2.8006e-01, -3.4511e-01,\n",
            "         2.0262e-01,  1.5437e+00, -2.6715e+00, -9.1445e-01, -2.3356e-01,\n",
            "        -1.9462e+00, -4.1053e-01,  8.1171e-02, -1.0918e+00, -4.1136e-01,\n",
            "        -1.1407e+00,  1.0451e-01, -5.2694e-01,  5.7935e-01,  7.6280e-01,\n",
            "        -1.2994e+00,  7.7864e-01, -7.8205e-01, -2.7830e-01, -1.3250e+00,\n",
            "         8.1193e-02,  6.1194e-01, -6.2774e-01, -2.1477e-01, -7.9760e-01,\n",
            "        -4.6627e-02, -7.1362e-01, -7.1642e-01, -8.0699e-01, -6.3257e-01,\n",
            "        -9.7300e-01,  1.8984e-01,  1.7074e-01, -7.5729e-01,  1.5901e-01,\n",
            "         2.3333e-01, -3.1389e-01,  6.2615e-01,  1.2846e+00,  7.0518e-02,\n",
            "         1.8375e-01, -1.5658e+00, -1.1354e+00, -1.6138e+00, -1.1765e+00,\n",
            "        -9.1679e-01,  8.7636e-01, -9.4038e-05, -1.6163e+00, -1.6635e+00,\n",
            "         1.7263e-01, -9.9273e-01, -5.5221e-01, -1.1657e-01, -3.6877e-01,\n",
            "        -1.7701e+00, -7.7440e-01, -9.2824e-01,  8.2341e-01,  7.6787e-01,\n",
            "        -1.7672e+00, -5.4156e-01,  5.2002e-01, -8.6848e-01, -3.6767e-01,\n",
            "         1.6672e-01, -2.9355e+00, -1.0629e+00, -1.6646e-01, -4.0312e-01,\n",
            "        -8.9350e-01, -3.5080e-01,  5.4658e-01, -1.2989e-01,  4.9549e-01,\n",
            "         7.1015e-01, -1.1884e+00, -1.6485e+00, -9.8315e-01, -5.7105e-02,\n",
            "         1.5601e-01, -4.2812e-01, -5.9673e-01, -1.3278e+00,  5.5761e-02,\n",
            "        -1.1181e+00])\n",
            "bn3.running_var: tensor([0.6513, 0.7060, 1.0781, 0.5282, 0.6765, 1.1480, 0.7999, 0.8019, 1.0896,\n",
            "        0.8481, 0.8351, 0.7778, 0.6665, 0.6968, 1.5584, 0.6914, 0.6989, 0.7442,\n",
            "        1.1581, 0.6530, 0.7346, 0.6422, 1.3930, 1.0671, 0.8949, 0.6739, 0.6215,\n",
            "        0.9982, 0.9688, 0.6476, 0.8884, 0.7724, 0.7826, 0.9679, 0.7652, 0.9752,\n",
            "        1.0360, 0.8223, 0.6929, 0.7186, 0.6150, 0.9996, 0.7399, 0.7494, 0.6442,\n",
            "        0.9835, 0.7783, 0.8085, 0.9124, 0.7378, 1.0368, 0.7730, 0.9724, 0.6493,\n",
            "        0.9626, 0.8854, 0.7634, 0.8677, 0.7051, 0.9461, 1.1695, 0.8361, 0.7403,\n",
            "        0.8082, 0.7684, 0.9747, 0.6756, 0.7850, 0.7112, 0.7520, 0.8483, 1.2722,\n",
            "        0.7300, 0.8661, 0.8149, 1.2124, 0.6836, 0.8494, 0.6919, 0.5668, 0.5763,\n",
            "        0.6693, 1.1829, 0.8561, 0.7615, 0.9585, 0.7972, 0.7105, 1.0252, 0.6945,\n",
            "        0.6799, 0.7223, 1.0155, 0.7956, 0.9012, 0.7330, 0.6537, 0.8841, 1.1319,\n",
            "        0.7137, 1.0802, 0.8852, 0.7445, 0.8562, 0.7152, 0.9077, 0.9450, 0.7535,\n",
            "        0.4795, 0.7898, 0.7539, 0.9560, 0.6178, 0.5872, 0.5779, 0.7287, 0.7050,\n",
            "        0.9345, 0.5959, 0.9151, 1.0308, 0.5896, 0.6960, 0.7005, 0.7541, 1.0498,\n",
            "        0.5276, 0.8520, 0.7819, 0.6943, 0.7206, 0.9845, 0.8573, 0.6912, 1.1465,\n",
            "        0.7600, 0.9163, 0.6439, 0.7908, 0.5351, 0.5998, 0.6433, 0.7473, 0.6678,\n",
            "        0.5145, 0.7155, 1.0805, 0.7417, 0.7270, 0.6901, 0.7665, 0.9106, 1.0229,\n",
            "        0.6582, 0.9561, 0.8916, 0.6315, 0.8295, 0.7749, 0.7386, 0.9473, 0.6414,\n",
            "        0.9821, 0.9570, 0.6645, 0.8446, 1.1405, 0.7090, 0.5742, 0.7603, 0.9486,\n",
            "        0.6810, 0.9297, 1.1944, 0.6460, 1.0611, 1.2624, 0.8007, 0.6921, 0.7222,\n",
            "        0.7594, 0.8544, 0.6047, 1.0132, 0.6746, 0.7270, 0.8073, 0.9365, 0.8352,\n",
            "        0.5376, 0.6674, 0.9292, 0.6421, 0.7173, 0.7081, 0.8160, 0.8287, 0.6099,\n",
            "        0.7494, 0.7606, 1.1466, 0.7705, 0.7023, 0.9404, 0.8469, 1.0960, 0.8123,\n",
            "        0.4831, 0.6078, 1.0430, 0.9043, 0.8806, 0.5815, 0.8129, 0.7054, 1.1045,\n",
            "        0.6009, 0.5211, 0.7520, 0.9586, 0.8763, 1.0276, 0.6561, 1.2190, 0.6141,\n",
            "        0.6914, 0.6665, 0.7287, 0.8490, 0.8111, 0.8739, 0.9860, 1.1889, 1.0144,\n",
            "        1.0621, 0.7517, 0.8830, 0.8217, 0.7299, 0.5486, 0.7868, 0.6558, 0.7745,\n",
            "        0.7140, 0.6615, 0.6984, 0.8304, 1.0326, 1.1986, 0.5384, 0.6053, 0.7601,\n",
            "        0.6756, 1.2884, 0.6869, 0.8721])\n",
            "bn3.num_batches_tracked: 5100\n",
            "fc1.weight: tensor([[-0.0073, -0.0076, -0.0060,  ..., -0.0056,  0.0085, -0.0116],\n",
            "        [ 0.0137,  0.0097, -0.0059,  ..., -0.0071, -0.0087, -0.0141],\n",
            "        [ 0.0270, -0.0110,  0.0082,  ...,  0.0151,  0.0038, -0.0033],\n",
            "        ...,\n",
            "        [-0.0118, -0.0070,  0.0100,  ..., -0.0109, -0.0046, -0.0155],\n",
            "        [-0.0151,  0.0105, -0.0026,  ...,  0.0069,  0.0126,  0.0036],\n",
            "        [-0.0157, -0.0151,  0.0093,  ..., -0.0069, -0.0135, -0.0134]])\n",
            "fc1.bias: tensor([-0.0107,  0.0042, -0.0040,  ...,  0.0029,  0.0061,  0.0028])\n",
            "fc2.weight: tensor([[-1.6445e-02, -1.8173e-02, -2.0753e-02,  ..., -2.8317e-03,\n",
            "          4.1223e-03,  3.0053e-02],\n",
            "        [-2.1368e-02, -2.3869e-02,  4.5334e-02,  ..., -3.5522e-03,\n",
            "         -3.0697e-02,  1.7144e-02],\n",
            "        [-2.6571e-02, -4.7809e-05,  4.0527e-03,  ..., -2.6165e-02,\n",
            "         -1.5566e-02,  1.4136e-02],\n",
            "        ...,\n",
            "        [-2.2725e-02, -4.8161e-03,  1.2837e-02,  ...,  8.8333e-03,\n",
            "         -8.8995e-03,  2.5101e-02],\n",
            "        [-2.2678e-02, -2.5981e-02, -1.7312e-02,  ..., -1.8817e-02,\n",
            "         -2.7212e-02, -1.1000e-02],\n",
            "        [ 1.4135e-02,  1.3867e-02, -2.0009e-02,  ...,  2.5036e-02,\n",
            "          4.7183e-03,  2.1760e-02]])\n",
            "fc2.bias: tensor([-0.0251, -0.0159,  0.0037,  0.0145, -0.0274,  0.0026,  0.0201, -0.0173,\n",
            "         0.0323, -0.0160,  0.0154,  0.0386, -0.0109, -0.0138,  0.0356,  0.0296,\n",
            "        -0.0148,  0.0077,  0.0051,  0.0182,  0.0235,  0.0023,  0.0537, -0.0159,\n",
            "         0.0148, -0.0068, -0.0328, -0.0255,  0.0066,  0.0411,  0.0259,  0.0270,\n",
            "        -0.0052, -0.0054, -0.0148, -0.0108, -0.0210, -0.0218, -0.0099,  0.0082,\n",
            "         0.0281,  0.0312,  0.0289, -0.0103,  0.0464,  0.0288,  0.0039,  0.0293,\n",
            "         0.0551,  0.0239,  0.0141,  0.0158, -0.0110,  0.0139, -0.0098,  0.0095,\n",
            "        -0.0255,  0.0650, -0.0004,  0.0115,  0.0165,  0.0170, -0.0015, -0.0202,\n",
            "         0.0354,  0.0261,  0.0190,  0.0069,  0.0089,  0.0617, -0.0186,  0.0004,\n",
            "         0.0585, -0.0105,  0.0061, -0.0255,  0.0137, -0.0210,  0.0457,  0.0050,\n",
            "         0.0283, -0.0049, -0.0167,  0.0344,  0.0221,  0.0043,  0.0475,  0.0294,\n",
            "         0.0729,  0.0117,  0.0209,  0.0085,  0.0088,  0.0091,  0.0212,  0.0329,\n",
            "         0.0100,  0.0324,  0.0492,  0.0065,  0.0228,  0.0069,  0.0174, -0.0051,\n",
            "        -0.0245, -0.0039, -0.0176,  0.0252,  0.0281,  0.0045, -0.0277,  0.0339,\n",
            "         0.0405,  0.0063,  0.0514,  0.0459,  0.0450, -0.0143,  0.0336,  0.0175,\n",
            "        -0.0002, -0.0186,  0.0191, -0.0119, -0.0225,  0.0063, -0.0233, -0.0036,\n",
            "         0.0396,  0.0329,  0.0273, -0.0003,  0.0239,  0.0019,  0.0066,  0.0215,\n",
            "         0.0689,  0.0038,  0.0339, -0.0319,  0.0158,  0.0237,  0.0172,  0.0641,\n",
            "         0.0019,  0.0102, -0.0229, -0.0132,  0.0219,  0.0203,  0.0239, -0.0197,\n",
            "        -0.0132, -0.0155,  0.0225,  0.0158,  0.0249,  0.0283,  0.0393, -0.0242,\n",
            "         0.0496,  0.0219,  0.0304,  0.0382, -0.0099,  0.0333,  0.0054,  0.0178,\n",
            "        -0.0095,  0.0129, -0.0009,  0.0179,  0.0202,  0.0489, -0.0184,  0.0223,\n",
            "         0.0079,  0.0260,  0.0173,  0.0433,  0.0190,  0.0115, -0.0111,  0.0211,\n",
            "        -0.0238,  0.0489, -0.0054, -0.0109,  0.0259,  0.0015,  0.0616, -0.0027,\n",
            "         0.0330, -0.0099,  0.0094,  0.0497,  0.0179,  0.0189, -0.0112,  0.0184,\n",
            "         0.0317, -0.0208, -0.0272,  0.0493,  0.0513, -0.0192,  0.0330, -0.0129,\n",
            "         0.0349,  0.0319,  0.0117,  0.0074,  0.0485, -0.0160,  0.0359, -0.0105,\n",
            "        -0.0035,  0.0370,  0.0275, -0.0017, -0.0326,  0.0232,  0.0779, -0.0042,\n",
            "         0.0156, -0.0224,  0.0228,  0.0011,  0.0144,  0.0128,  0.0290,  0.0141,\n",
            "        -0.0304, -0.0143,  0.0379, -0.0155,  0.0686,  0.0345,  0.0549,  0.0512,\n",
            "         0.0019,  0.0060,  0.0148,  0.0092, -0.0333, -0.0006,  0.0027,  0.0217,\n",
            "        -0.0226, -0.0146,  0.0039,  0.0193,  0.0390, -0.0138,  0.0205,  0.0110,\n",
            "         0.0247,  0.0057,  0.0015,  0.0313,  0.0076, -0.0123, -0.0006,  0.0009,\n",
            "         0.0146, -0.0017,  0.0006,  0.0525, -0.0293, -0.0082, -0.0278, -0.0072,\n",
            "         0.0557,  0.0113,  0.0338,  0.0264, -0.0267,  0.0645, -0.0319, -0.0236,\n",
            "         0.0578, -0.0392,  0.0292,  0.0208, -0.0001,  0.0349,  0.0366,  0.0083,\n",
            "        -0.0051,  0.0541,  0.0106,  0.0244,  0.0337,  0.0292, -0.0261,  0.0178,\n",
            "         0.0496, -0.0237,  0.0371, -0.0202,  0.0440,  0.0403, -0.0184,  0.0370,\n",
            "         0.0502,  0.0342,  0.0597, -0.0288,  0.0213, -0.0054,  0.0347, -0.0258,\n",
            "         0.0292,  0.0370,  0.0126,  0.0382, -0.0137,  0.0496, -0.0319, -0.0082,\n",
            "         0.0270, -0.0108,  0.0142, -0.0029,  0.0017, -0.0130, -0.0118,  0.0490,\n",
            "         0.0348,  0.0185,  0.0178,  0.0072,  0.0048,  0.0289,  0.0218, -0.0341,\n",
            "         0.0165,  0.0157, -0.0022, -0.0385,  0.0122,  0.0020, -0.0074, -0.0106,\n",
            "         0.0097, -0.0075,  0.0029, -0.0054, -0.0084,  0.0301,  0.0303,  0.0380,\n",
            "         0.0159,  0.0128,  0.0346,  0.0064,  0.0385,  0.0745, -0.0104,  0.0255,\n",
            "         0.0162,  0.0098,  0.0211, -0.0281, -0.0034,  0.0031,  0.0314,  0.0106,\n",
            "         0.0198, -0.0005, -0.0190, -0.0157,  0.0328, -0.0188, -0.0165,  0.0143,\n",
            "        -0.0234,  0.0262,  0.0424,  0.0277,  0.0178,  0.0139,  0.0338,  0.0304,\n",
            "         0.0779, -0.0008, -0.0013,  0.0603,  0.0148,  0.0251,  0.0160, -0.0219,\n",
            "         0.0280,  0.0158,  0.0109, -0.0138,  0.0303, -0.0222,  0.0487, -0.0287,\n",
            "         0.0288,  0.0457,  0.0306, -0.0107, -0.0059,  0.0012,  0.0150, -0.0200,\n",
            "        -0.0049,  0.0435, -0.0204,  0.0531,  0.0368,  0.0058, -0.0148,  0.0476,\n",
            "         0.0226,  0.0284,  0.0153, -0.0129, -0.0013,  0.0456,  0.0169,  0.0019,\n",
            "         0.0182,  0.0017,  0.0248,  0.0710, -0.0140,  0.0144,  0.0124,  0.0541,\n",
            "         0.0347,  0.0314,  0.0065, -0.0256,  0.0506, -0.0122,  0.0148,  0.0029,\n",
            "         0.0140,  0.0791,  0.0049, -0.0161,  0.0526,  0.0201, -0.0055, -0.0124,\n",
            "        -0.0119, -0.0166, -0.0007, -0.0030,  0.0130, -0.0040,  0.0206, -0.0110,\n",
            "        -0.0133,  0.0226,  0.0347, -0.0002, -0.0119,  0.0074, -0.0325,  0.0348,\n",
            "         0.0015,  0.0056, -0.0387, -0.0145,  0.0380,  0.0221, -0.0019,  0.0659,\n",
            "         0.0106, -0.0047,  0.0280,  0.0246, -0.0016, -0.0351,  0.0159, -0.0055,\n",
            "         0.0762,  0.0500,  0.0344,  0.0240, -0.0137, -0.0151,  0.0220, -0.0303,\n",
            "        -0.0242,  0.0064, -0.0124, -0.0199, -0.0163,  0.0023, -0.0021,  0.0586,\n",
            "        -0.0095,  0.0547, -0.0353,  0.0183, -0.0135,  0.0170, -0.0042, -0.0200,\n",
            "        -0.0035,  0.0271, -0.0263,  0.0051,  0.0356,  0.0413,  0.0281,  0.0357])\n",
            "fc3.weight: tensor([[ 0.0280,  0.0626,  0.0258,  ...,  0.0682,  0.0372, -0.1043],\n",
            "        [-0.0340,  0.0170, -0.1276,  ..., -0.1156,  0.0216, -0.1180],\n",
            "        [-0.0036,  0.0585, -0.0175,  ...,  0.0832, -0.0116,  0.1045],\n",
            "        ...,\n",
            "        [-0.0436, -0.0879, -0.0257,  ..., -0.0676, -0.0053,  0.0419],\n",
            "        [-0.0394, -0.0043,  0.0102,  ..., -0.0113, -0.0223, -0.0249],\n",
            "        [-0.0437, -0.0047, -0.0871,  ..., -0.1308,  0.0111, -0.0470]])\n",
            "fc3.bias: tensor([ 0.0566, -0.2630, -0.0912,  0.3728,  0.1476, -0.1719, -0.0046,  0.0683,\n",
            "        -0.1899, -0.0439])\n",
            "Checking conv1 weights after loading:\n",
            "tensor([[[[ 0.1532,  0.2262,  0.1518],\n",
            "          [-0.1939, -0.2444, -0.0565],\n",
            "          [-0.1329, -0.2885, -0.1596]],\n",
            "\n",
            "         [[-0.0299,  0.2585,  0.1343],\n",
            "          [-0.1591,  0.1495,  0.2431],\n",
            "          [-0.1123, -0.0622, -0.0253]],\n",
            "\n",
            "         [[-0.0038, -0.0171,  0.0136],\n",
            "          [ 0.0598,  0.1440, -0.1341],\n",
            "          [ 0.1565, -0.1545,  0.0560]]],\n",
            "\n",
            "\n",
            "        [[[-0.0560, -0.2677, -0.0152],\n",
            "          [-0.3005, -0.2657, -0.2643],\n",
            "          [-0.1437, -0.1792, -0.2201]],\n",
            "\n",
            "         [[ 0.1807,  0.3710,  0.0316],\n",
            "          [ 0.4806,  0.6360,  0.4813],\n",
            "          [ 0.2348,  0.3678,  0.3855]],\n",
            "\n",
            "         [[-0.0407, -0.2538,  0.0169],\n",
            "          [-0.3217, -0.2495, -0.3543],\n",
            "          [-0.0284, -0.4016, -0.1158]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0054,  0.0565,  0.0595],\n",
            "          [ 0.1971, -0.0149, -0.2864],\n",
            "          [-0.0724,  0.1028, -0.0829]],\n",
            "\n",
            "         [[ 0.1058,  0.2043, -0.2594],\n",
            "          [ 0.2741, -0.0642, -0.1653],\n",
            "          [ 0.1833, -0.0568, -0.2118]],\n",
            "\n",
            "         [[ 0.2582, -0.0627, -0.1724],\n",
            "          [ 0.0827,  0.0415, -0.1085],\n",
            "          [ 0.2359, -0.1181, -0.2380]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.3017,  0.2462,  0.1065],\n",
            "          [ 0.3258, -0.2903, -0.2470],\n",
            "          [-0.0052, -0.3644, -0.3301]],\n",
            "\n",
            "         [[ 0.0238, -0.2379, -0.0451],\n",
            "          [-0.0269, -0.1607,  0.2029],\n",
            "          [ 0.1282,  0.0504,  0.1970]],\n",
            "\n",
            "         [[-0.1761, -0.1161, -0.0581],\n",
            "          [-0.1644, -0.0028,  0.1290],\n",
            "          [-0.0969,  0.0391,  0.3500]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1231, -0.1953, -0.1040],\n",
            "          [ 0.1350,  0.1959, -0.2623],\n",
            "          [-0.0322,  0.1045,  0.1441]],\n",
            "\n",
            "         [[-0.0161, -0.1821,  0.0909],\n",
            "          [ 0.1384,  0.1617, -0.2822],\n",
            "          [-0.1715,  0.2259,  0.0616]],\n",
            "\n",
            "         [[-0.1019, -0.0985, -0.0917],\n",
            "          [ 0.2044, -0.0564, -0.3139],\n",
            "          [ 0.0398,  0.3717, -0.0666]]],\n",
            "\n",
            "\n",
            "        [[[-0.1340,  0.1684,  0.1261],\n",
            "          [ 0.0301,  0.1927,  0.0225],\n",
            "          [ 0.1631, -0.1491, -0.2873]],\n",
            "\n",
            "         [[ 0.1753,  0.1475,  0.1032],\n",
            "          [ 0.0970, -0.0266,  0.0565],\n",
            "          [ 0.1466, -0.1547, -0.1612]],\n",
            "\n",
            "         [[ 0.1148, -0.2044, -0.0840],\n",
            "          [-0.2260,  0.0216, -0.1739],\n",
            "          [-0.0629, -0.1477, -0.2780]]]])\n",
            "Quantized conv1 weights:\n",
            "[[[[  4   6   4]\n",
            "   [ -5  -6  -1]\n",
            "   [ -3  -7  -4]]\n",
            "\n",
            "  [[ -1   7   3]\n",
            "   [ -4   4   6]\n",
            "   [ -3  -2  -1]]\n",
            "\n",
            "  [[  0   0   0]\n",
            "   [  2   4  -3]\n",
            "   [  4  -4   1]]]\n",
            "\n",
            "\n",
            " [[[ -1  -7   0]\n",
            "   [ -8  -7  -7]\n",
            "   [ -4  -5  -6]]\n",
            "\n",
            "  [[  5   9   1]\n",
            "   [ 12  16  12]\n",
            "   [  6   9  10]]\n",
            "\n",
            "  [[ -1  -6   0]\n",
            "   [ -8  -6  -9]\n",
            "   [ -1 -10  -3]]]\n",
            "\n",
            "\n",
            " [[[  0   1   2]\n",
            "   [  5   0  -7]\n",
            "   [ -2   3  -2]]\n",
            "\n",
            "  [[  3   5  -7]\n",
            "   [  7  -2  -4]\n",
            "   [  5  -1  -5]]\n",
            "\n",
            "  [[  7  -2  -4]\n",
            "   [  2   1  -3]\n",
            "   [  6  -3  -6]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[  8   6   3]\n",
            "   [  8  -7  -6]\n",
            "   [  0  -9  -8]]\n",
            "\n",
            "  [[  1  -6  -1]\n",
            "   [ -1  -4   5]\n",
            "   [  3   1   5]]\n",
            "\n",
            "  [[ -4  -3  -1]\n",
            "   [ -4   0   3]\n",
            "   [ -2   1   9]]]\n",
            "\n",
            "\n",
            " [[[  3  -5  -3]\n",
            "   [  3   5  -7]\n",
            "   [ -1   3   4]]\n",
            "\n",
            "  [[  0  -5   2]\n",
            "   [  4   4  -7]\n",
            "   [ -4   6   2]]\n",
            "\n",
            "  [[ -3  -3  -2]\n",
            "   [  5  -1  -8]\n",
            "   [  1   9  -2]]]\n",
            "\n",
            "\n",
            " [[[ -3   4   3]\n",
            "   [  1   5   1]\n",
            "   [  4  -4  -7]]\n",
            "\n",
            "  [[  4   4   3]\n",
            "   [  2  -1   1]\n",
            "   [  4  -4  -4]]\n",
            "\n",
            "  [[  3  -5  -2]\n",
            "   [ -6   1  -4]\n",
            "   [ -2  -4  -7]]]]\n",
            "NNgen variable w0 values:\n",
            "[[[[  4   6   4]\n",
            "   [ -5  -6  -1]\n",
            "   [ -3  -7  -4]]\n",
            "\n",
            "  [[ -1   7   3]\n",
            "   [ -4   4   6]\n",
            "   [ -3  -2  -1]]\n",
            "\n",
            "  [[  0   0   0]\n",
            "   [  2   4  -3]\n",
            "   [  4  -4   1]]]\n",
            "\n",
            "\n",
            " [[[ -1  -7   0]\n",
            "   [ -8  -7  -7]\n",
            "   [ -4  -5  -6]]\n",
            "\n",
            "  [[  5   9   1]\n",
            "   [ 12  16  12]\n",
            "   [  6   9  10]]\n",
            "\n",
            "  [[ -1  -6   0]\n",
            "   [ -8  -6  -9]\n",
            "   [ -1 -10  -3]]]\n",
            "\n",
            "\n",
            " [[[  0   1   2]\n",
            "   [  5   0  -7]\n",
            "   [ -2   3  -2]]\n",
            "\n",
            "  [[  3   5  -7]\n",
            "   [  7  -2  -4]\n",
            "   [  5  -1  -5]]\n",
            "\n",
            "  [[  7  -2  -4]\n",
            "   [  2   1  -3]\n",
            "   [  6  -3  -6]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[  8   6   3]\n",
            "   [  8  -7  -6]\n",
            "   [  0  -9  -8]]\n",
            "\n",
            "  [[  1  -6  -1]\n",
            "   [ -1  -4   5]\n",
            "   [  3   1   5]]\n",
            "\n",
            "  [[ -4  -3  -1]\n",
            "   [ -4   0   3]\n",
            "   [ -2   1   9]]]\n",
            "\n",
            "\n",
            " [[[  3  -5  -3]\n",
            "   [  3   5  -7]\n",
            "   [ -1   3   4]]\n",
            "\n",
            "  [[  0  -5   2]\n",
            "   [  4   4  -7]\n",
            "   [ -4   6   2]]\n",
            "\n",
            "  [[ -3  -3  -2]\n",
            "   [  5  -1  -8]\n",
            "   [  1   9  -2]]]\n",
            "\n",
            "\n",
            " [[[ -3   4   3]\n",
            "   [  1   5   1]\n",
            "   [  4  -4  -7]]\n",
            "\n",
            "  [[  4   4   3]\n",
            "   [  2  -1   1]\n",
            "   [  4  -4  -4]]\n",
            "\n",
            "  [[  3  -5  -2]\n",
            "   [ -6   1  -4]\n",
            "   [ -2  -4  -7]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random parameters"
      ],
      "metadata": {
        "id": "go6UOvjvOSeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import torch\n",
        "\n",
        "\n",
        "# # Assign quantized weights to the NNgen operators\n",
        "# # Layer 0\n",
        "# w0_value = np.clip(pytorch_model.conv1.weight.data.numpy(), -5.0, 5.0)\n",
        "# w0_value = (w0_value * (2.0 ** (weight_dtype.width - 1) - 1) / 5.0).astype(np.int64)\n",
        "# w0.set_value(w0_value)\n",
        "\n",
        "# b0_value = np.clip(pytorch_model.bn1.bias.data.numpy(), -5.0, 5.0)\n",
        "# b0_value = (b0_value * (2.0 ** (bias_dtype.width - 1) - 1) / 5.0 / 100.0).astype(np.int64)\n",
        "# b0.set_value(b0_value)\n",
        "\n",
        "# # Layer 1\n",
        "# w1_value = np.clip(pytorch_model.conv2.weight.data.numpy(), -5.0, 5.0)\n",
        "# w1_value = (w1_value * (2.0 ** (weight_dtype.width - 1) - 1) / 5.0).astype(np.int64)\n",
        "# w1.set_value(w1_value)\n",
        "\n",
        "# b1_value = np.clip(pytorch_model.bn2.bias.data.numpy(), -5.0, 5.0)\n",
        "# b1_value = (b1_value * (2.0 ** (bias_dtype.width - 1) - 1) / 5.0 / 100.0).astype(np.int64)\n",
        "# b1.set_value(b1_value)\n",
        "\n",
        "# # Layer 2\n",
        "# w2_value = np.clip(pytorch_model.conv3.weight.data.numpy(), -5.0, 5.0)\n",
        "# w2_value = (w2_value * (2.0 ** (weight_dtype.width - 1) - 1) / 5.0).astype(np.int64)\n",
        "# w2.set_value(w2_value)\n",
        "\n",
        "# b2_value = np.clip(pytorch_model.bn3.bias.data.numpy(), -5.0, 5.0)\n",
        "# b2_value = (b2_value * (2.0 ** (bias_dtype.width - 1) - 1) / 5.0 / 100.0).astype(np.int64)\n",
        "# b2.set_value(b2_value)\n",
        "\n",
        "# # Layer 3\n",
        "# w3_value = np.clip(pytorch_model.fc1.weight.data.numpy(), -5.0, 5.0)\n",
        "# w3_value = (w3_value * (2.0 ** (weight_dtype.width - 1) - 1) / 5.0).astype(np.int64)\n",
        "# w3.set_value(w3_value)\n",
        "\n",
        "# b3_value = np.clip(pytorch_model.fc1.bias.data.numpy(), -5.0, 5.0)\n",
        "# b3_value = (b3_value * (2.0 ** (bias_dtype.width - 1) - 1) / 5.0 / 100.0).astype(np.int64)\n",
        "# b3.set_value(b3_value)\n",
        "\n",
        "# # Layer 4\n",
        "# w4_value = np.clip(pytorch_model.fc2.weight.data.numpy(), -5.0, 5.0)\n",
        "# w4_value = (w4_value * (2.0 ** (weight_dtype.width - 1) - 1) / 5.0).astype(np.int64)\n",
        "# w4.set_value(w4_value)\n",
        "\n",
        "# b4_value = np.clip(pytorch_model.fc2.bias.data.numpy(), -5.0, 5.0)\n",
        "# b4_value = (b4_value * (2.0 ** (bias_dtype.width - 1) - 1) / 5.0 / 100.0).astype(np.int64)\n",
        "# b4.set_value(b4_value)\n",
        "\n",
        "# # Layer 5\n",
        "# w5_value = np.clip(pytorch_model.fc3.weight.data.numpy(), -5.0, 5.0)\n",
        "# w5_value = (w5_value * (2.0 ** (weight_dtype.width - 1) - 1) / 5.0).astype(np.int64)\n",
        "# w5.set_value(w5_value)\n",
        "\n",
        "# b5_value = np.clip(pytorch_model.fc3.bias.data.numpy(), -5.0, 5.0)\n",
        "# b5_value = (b5_value * (2.0 ** (bias_dtype.width - 1) - 1) / 5.0 / 100.0).astype(np.int64)\n",
        "# b5.set_value(b5_value)\n",
        "# output_layer = ng.matmul(a4, w5, bias=b5, scale=s5, transposed_b=True, name='output_layer', sum_dtype=ng.int64)\n",
        "\n",
        "# # Print a message indicating successful completion\n",
        "# print(\"Quantized weights assigned to all layers successfully.\")\n"
      ],
      "metadata": {
        "id": "W0IkBCtI6Jkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fd38e5-430f-417c-88b8-abb08b189f52"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized weights assigned to all layers successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HARDWARE ATTRIBUTES\n"
      ],
      "metadata": {
        "id": "EunMMbI9cCTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conv2d, matmul\n",
        "# par_ich: parallelism in input-channel\n",
        "# par_och: parallelism in output-channel\n",
        "# par_col: parallelism in pixel column\n",
        "# par_row: parallelism in pixel row\n",
        "# cshamt_out: right shift amount after applying bias/scale\n",
        "\n",
        "par = 2  # Example value for parallelism\n",
        "value_ram_size = 1024  # Example value for value RAM size\n",
        "out_ram_size = 512  # Example value for output RAM size\n",
        "\n",
        "par_ich = 2\n",
        "par_och = 2\n",
        "cshamt_out = weight_dtype.width + 1\n",
        "\n",
        "a0.attribute(par=par, value_ram_size=value_ram_size, out_ram_size=out_ram_size)\n",
        "a1.attribute(par=par, value_ram_size=value_ram_size, out_ram_size=out_ram_size)\n",
        "a2.attribute(par=par, value_ram_size=value_ram_size, out_ram_size=out_ram_size)\n",
        "output_layer.attribute(par_ich=par_ich, par_och=par_och,\n",
        "                       cshamt_out=weight_dtype.width + 1)\n",
        "\n",
        "# max_pool\n",
        "# par: parallelism in in/out channel\n",
        "\n"
      ],
      "metadata": {
        "id": "46TCcwQycGUw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING AND VERIFICATION"
      ],
      "metadata": {
        "id": "8yqp3Rt2cJqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer_value = np.random.normal(size=input_layer.length).reshape(input_layer.shape)\n",
        "input_layer_value = np.clip(input_layer_value, -5.0, 5.0)\n",
        "input_layer_value = input_layer_value * (2.0 ** (input_layer.dtype.width - 1) - 1) / 5.0\n",
        "input_layer_value = np.round(input_layer_value).astype(np.int64)\n",
        "\n",
        "print(output_layer.value)\n",
        "eval_outs = ng.eval([output_layer], input_layer=input_layer_value)\n",
        "output_layer_value = eval_outs[0]\n",
        "\n",
        "print(output_layer_value)\n"
      ],
      "metadata": {
        "id": "5hY2PKwXcOEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee482b43-0ee6-4a84-9f42-e84f6b176937"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "[[ 3 -4  1 -2 -6 -6  0  4 -1 -7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([output_layer])"
      ],
      "metadata": {
        "id": "OZ3EliEjvOKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5cefbd-9a9d-439f-bd6e-e424488ec569"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<nngen.operator.matmul.matmul object at 0x7d67a1937dc0>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VERILOG CONVERSION"
      ],
      "metadata": {
        "id": "HWcurfmicT7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "silent = False\n",
        "axi_datawidth = 32\n",
        "\n",
        "param_filename = 'complexcnn.npy'\n",
        "chunk_size = 32\n",
        "\n",
        "# Export the parameters\n",
        "param_data = ng.export_ndarray([output_layer], chunk_size)\n",
        "\n",
        "print(param_data)\n",
        "np.save(param_filename, param_data)\n",
        "\n",
        "# to Veriloggen object\n",
        "try:\n",
        "    targ = ng.to_veriloggen([output_layer], 'complexcnn', silent=silent, config={'maxi_datawidth': axi_datawidth})\n",
        "    print(\"Veriloggen object generated successfully\")\n",
        "except Exception as e:\n",
        "    print(\"An error occurred during Veriloggen generation:\")\n",
        "    print(str(e))\n"
      ],
      "metadata": {
        "id": "v3ApuJ-qcTp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68d515d-47de-4674-e421-fe5347a41c3e",
        "collapsed": true
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[235   3 202 ...   0   0   0]\n",
            "NNgen: Neural Network Accelerator Generator (version 1.3.4)\n",
            "[Configuration]\n",
            "(AXI Master Interface)\n",
            "  Data width   : 32\n",
            "  Address width: 32\n",
            "(AXI Slave Interface)\n",
            "  Data width   : 32\n",
            "  Address width: 32\n",
            "[Schedule Table]\n",
            "(Stage 0)\n",
            "(Stage 1)\n",
            "  <conv2d None dtype:int16 shape:(1, 32, 32, 64) strides:(1, 1, 1, 1) padding:'SAME'-(1, 1, 1, 1) bias:(64,) scale:(64,) act_func:relu sum_dtype:int64 concur_och:8 stationary:filter keep_input default_addr:10205632 g_index:0 l_index:1 word_alignment:2 aligned_shape:(1, 32, 32, 64) scale_factor:1.000000>\n",
            "  | <placeholder input_layer dtype:int16 shape:(1, 32, 32, 3) default_addr:64 g_index:2 word_alignment:2 aligned_shape:(1, 32, 32, 4) scale_factor:1.000000>\n",
            "  | <variable w0 dtype:int16 shape:(64, 3, 3, 3) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(64, 3, 3, 4) scale_factor:1.000000>\n",
            "  | <variable b0 dtype:int16 shape:(64,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(64,) scale_factor:1.000000>\n",
            "  | <variable s0 dtype:int16 shape:(64,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(64,) scale_factor:1.000000>\n",
            "(Stage 2)\n",
            "  <max_pool_serial None dtype:int16 shape:(1, 16, 16, 64) ksize:(1, 2, 2, 1) strides:(1, 2, 2, 1) padding:'SAME'-(0, 0, 0, 0) par:2 value_ram_size:1024 out_ram_size:512 no_reuse default_addr:10336704 g_index:0 l_index:2 word_alignment:2 aligned_shape:(1, 16, 16, 64) scale_factor:1.000000>\n",
            "  | <conv2d None dtype:int16 shape:(1, 32, 32, 64) strides:(1, 1, 1, 1) padding:'SAME'-(1, 1, 1, 1) bias:(64,) scale:(64,) act_func:relu sum_dtype:int64 concur_och:8 stationary:filter keep_input default_addr:10205632 g_index:0 l_index:1 word_alignment:2 aligned_shape:(1, 32, 32, 64) scale_factor:1.000000>\n",
            "(Stage 3)\n",
            "  <conv2d None dtype:int16 shape:(1, 16, 16, 128) strides:(1, 1, 1, 1) padding:'SAME'-(1, 1, 1, 1) bias:(128,) scale:(128,) act_func:relu sum_dtype:int64 concur_och:4 stationary:filter default_addr:10369472 g_index:0 l_index:3 word_alignment:2 aligned_shape:(1, 16, 16, 128) scale_factor:1.000000>\n",
            "  | <max_pool_serial None dtype:int16 shape:(1, 16, 16, 64) ksize:(1, 2, 2, 1) strides:(1, 2, 2, 1) padding:'SAME'-(0, 0, 0, 0) par:2 value_ram_size:1024 out_ram_size:512 no_reuse default_addr:10336704 g_index:0 l_index:2 word_alignment:2 aligned_shape:(1, 16, 16, 64) scale_factor:1.000000>\n",
            "  | <variable w1 dtype:int16 shape:(128, 3, 3, 64) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(128, 3, 3, 64) scale_factor:1.000000>\n",
            "  | <variable b1 dtype:int16 shape:(128,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(128,) scale_factor:1.000000>\n",
            "  | <variable s1 dtype:int16 shape:(128,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(128,) scale_factor:1.000000>\n",
            "(Stage 4)\n",
            "  <max_pool_serial None dtype:int16 shape:(1, 8, 8, 128) ksize:(1, 2, 2, 1) strides:(1, 2, 2, 1) padding:'SAME'-(0, 0, 0, 0) par:2 value_ram_size:1024 out_ram_size:512 no_reuse default_addr:10435008 g_index:0 l_index:4 word_alignment:2 aligned_shape:(1, 8, 8, 128) scale_factor:1.000000>\n",
            "  | <conv2d None dtype:int16 shape:(1, 16, 16, 128) strides:(1, 1, 1, 1) padding:'SAME'-(1, 1, 1, 1) bias:(128,) scale:(128,) act_func:relu sum_dtype:int64 concur_och:4 stationary:filter default_addr:10369472 g_index:0 l_index:3 word_alignment:2 aligned_shape:(1, 16, 16, 128) scale_factor:1.000000>\n",
            "(Stage 5)\n",
            "  <conv2d None dtype:int16 shape:(1, 8, 8, 256) strides:(1, 1, 1, 1) padding:'SAME'-(1, 1, 1, 1) bias:(256,) scale:(256,) act_func:relu sum_dtype:int64 concur_och:2 stationary:filter default_addr:10451392 g_index:0 l_index:5 word_alignment:2 aligned_shape:(1, 8, 8, 256) scale_factor:1.000000>\n",
            "  | <max_pool_serial None dtype:int16 shape:(1, 8, 8, 128) ksize:(1, 2, 2, 1) strides:(1, 2, 2, 1) padding:'SAME'-(0, 0, 0, 0) par:2 value_ram_size:1024 out_ram_size:512 no_reuse default_addr:10435008 g_index:0 l_index:4 word_alignment:2 aligned_shape:(1, 8, 8, 128) scale_factor:1.000000>\n",
            "  | <variable w2 dtype:int16 shape:(256, 3, 3, 128) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(256, 3, 3, 128) scale_factor:1.000000>\n",
            "  | <variable b2 dtype:int16 shape:(256,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(256,) scale_factor:1.000000>\n",
            "  | <variable s2 dtype:int16 shape:(256,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(256,) scale_factor:1.000000>\n",
            "(Stage 6)\n",
            "  <max_pool_serial None dtype:int16 shape:(1, 4, 4, 256) ksize:(1, 2, 2, 1) strides:(1, 2, 2, 1) padding:'SAME'-(0, 0, 0, 0) par:2 value_ram_size:1024 out_ram_size:512 no_reuse default_addr:10484160 g_index:0 l_index:6 word_alignment:2 aligned_shape:(1, 4, 4, 256) scale_factor:1.000000>\n",
            "  | <conv2d None dtype:int16 shape:(1, 8, 8, 256) strides:(1, 1, 1, 1) padding:'SAME'-(1, 1, 1, 1) bias:(256,) scale:(256,) act_func:relu sum_dtype:int64 concur_och:2 stationary:filter default_addr:10451392 g_index:0 l_index:5 word_alignment:2 aligned_shape:(1, 8, 8, 256) scale_factor:1.000000>\n",
            "(Stage 7)\n",
            "  <_lazy_reshape None dtype:int16 shape:(1, 4096) alias_of:<max_pool_serial> default_addr:10484160 g_index:0 l_index:6 word_alignment:2 aligned_shape:(1, 4096) scale_factor:1.000000>\n",
            "  | <max_pool_serial None dtype:int16 shape:(1, 4, 4, 256) ksize:(1, 2, 2, 1) strides:(1, 2, 2, 1) padding:'SAME'-(0, 0, 0, 0) par:2 value_ram_size:1024 out_ram_size:512 no_reuse default_addr:10484160 g_index:0 l_index:6 word_alignment:2 aligned_shape:(1, 4, 4, 256) scale_factor:1.000000>\n",
            "(Stage 8)\n",
            "  <matmul None dtype:int16 shape:(1, 1024) bias:(1024,) scale:(1024,) act_func:relu sum_dtype:int64 concur_out_col:2 stationary:right keep_left default_addr:10492352 g_index:0 l_index:7 word_alignment:2 aligned_shape:(1, 1024) scale_factor:1.000000>\n",
            "  | <_lazy_reshape None dtype:int16 shape:(1, 4096) alias_of:<max_pool_serial> default_addr:10484160 g_index:0 l_index:6 word_alignment:2 aligned_shape:(1, 4096) scale_factor:1.000000>\n",
            "  | <variable w3 dtype:int16 shape:(1024, 4096) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(1024, 4096) scale_factor:1.000000>\n",
            "  | <variable b3 dtype:int16 shape:(1024,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(1024,) scale_factor:1.000000>\n",
            "  | <variable s3 dtype:int16 shape:(1024,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(1024,) scale_factor:1.000000>\n",
            "(Stage 9)\n",
            "  <matmul None dtype:int16 shape:(1, 512) bias:(512,) scale:(512,) act_func:relu sum_dtype:int64 concur_out_col:8 stationary:right keep_left default_addr:10494400 g_index:0 l_index:8 word_alignment:2 aligned_shape:(1, 512) scale_factor:1.000000>\n",
            "  | <matmul None dtype:int16 shape:(1, 1024) bias:(1024,) scale:(1024,) act_func:relu sum_dtype:int64 concur_out_col:2 stationary:right keep_left default_addr:10492352 g_index:0 l_index:7 word_alignment:2 aligned_shape:(1, 1024) scale_factor:1.000000>\n",
            "  | <variable w4 dtype:int16 shape:(512, 1024) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(512, 1024) scale_factor:1.000000>\n",
            "  | <variable b4 dtype:int16 shape:(512,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(512,) scale_factor:1.000000>\n",
            "  | <variable s4 dtype:int16 shape:(512,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(512,) scale_factor:1.000000>\n",
            "(Stage 10)\n",
            "  <matmul output_layer dtype:int16 shape:(1, 10) bias:(10,) scale:(10,) cshamt_out:17 sum_dtype:int64 par_left_col:2 par_out_col:2 concur_out_col:2 stationary:right keep_left default_addr:0 g_index:1 word_alignment:2 aligned_shape:(1, 10) scale_factor:1.000000>\n",
            "  | <matmul None dtype:int16 shape:(1, 512) bias:(512,) scale:(512,) act_func:relu sum_dtype:int64 concur_out_col:8 stationary:right keep_left default_addr:10494400 g_index:0 l_index:8 word_alignment:2 aligned_shape:(1, 512) scale_factor:1.000000>\n",
            "  | <variable w5 dtype:int16 shape:(10, 512) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(10, 512) scale_factor:1.000000>\n",
            "  | <variable b5 dtype:int16 shape:(10,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(10,) scale_factor:1.000000>\n",
            "  | <variable s5 dtype:int16 shape:(10,) default_addr:8256 g_index:3 word_alignment:2 aligned_shape:(10,) scale_factor:1.000000>\n",
            "[RAM (spec: num)]\n",
            "  32-bit 4096-entry 2-port 1-bank RAM: 1\n",
            "  32-bit 1024-entry 2-port 1-bank RAM: 1\n",
            "  32-bit 512-entry 2-port 1-bank RAM: 2\n",
            "  32-bit 128-entry 2-port 1-bank RAM: 3\n",
            "  16-bit 16384-entry 2-port 2-bank RAM: 1\n",
            "  16-bit 4096-entry 2-port 2-bank RAM: 1\n",
            "  16-bit 1024-entry 2-port 2-bank RAM: 11\n",
            "  16-bit 512-entry 2-port 2-bank RAM: 13\n",
            "[Substream (spec: num)]\n",
            "  ('acc_rshift_round_frac', (64, 0, True, 64, 0, True)): 2\n",
            "  ('add_tree', (64, 0, True, 1)): 1\n",
            "  ('add_tree', (64, 0, True, 2)): 2\n",
            "  ('add_tree', (64, 0, True, 9)): 1\n",
            "  ('mul_rshift_round_clip', (64, 0, True, 16, 0, True, 80, 0, True, 16, 0, True, False)): 2\n",
            "  ('mul_rshift_round_madd', (16, 0, True, 16, 0, True, 32, 0, True)): 9\n",
            "  ('reduce_max', (16, 0, True)): 2\n",
            "[Stream (spec: num)]\n",
            "  (((<class 'nngen.operator.conv2d.conv2d'>, <dtype int16>, <dtype int16>, <dtype int16>, <dtype int16>), <dtype int16>, 1), 3, 3, False, None, <dtype int64>, 1, 1, 1, 1, 9, 9): 1\n",
            "  (((<class 'nngen.operator.pool_serial.max_pool_serial'>, <dtype int16>), <dtype int16>, 2), 2, 2, True, 2): 1\n",
            "  (((<class 'nngen.operator.basic._lazy_reshape'>, <dtype int16>), <dtype int16>, 1), True): 1\n",
            "  (((<class 'nngen.operator.matmul.matmul'>, <dtype int16>, <dtype int16>, <dtype int16>, <dtype int16>), <dtype int16>, 1), 1, 1, False, None, <dtype int64>, 1, 1, 1, 1, 1, 1): 1\n",
            "  (((<class 'nngen.operator.matmul.matmul'>, <dtype int16>, <dtype int16>, <dtype int16>, <dtype int16>), <dtype int16>, 1), 1, 1, False, None, <dtype int64>, 2, 2, 1, 1, 1, 4): 1\n",
            "[State IDs in main_fsm]\n",
            "  (3, 4, 'input_layer', 'None')\n",
            "  (12, 14, None, 'control_conv2d_73')\n",
            "  (19, 21, None, 'control_max_pool_serial_75')\n",
            "  (29, 31, None, 'control_conv2d_73')\n",
            "  (36, 38, None, 'control_max_pool_serial_75')\n",
            "  (46, 48, None, 'control_conv2d_73')\n",
            "  (53, 55, None, 'control_max_pool_serial_75')\n",
            "  (56, 57, None, 'None')\n",
            "  (65, 67, None, 'control_matmul_92')\n",
            "  (75, 77, None, 'control_matmul_92')\n",
            "  (84, 86, 'output_layer', 'control_matmul_103')\n",
            "[Control (name (# states: num))]\n",
            "  main_fsm (# states: 92)\n",
            "  control_conv2d_73 (# states: 35)\n",
            "  control_max_pool_serial_75 (# states: 20)\n",
            "  control_matmul_92 (# states: 29)\n",
            "  control_matmul_103 (# states: 29)\n",
            "[Register Map]\n",
            "    0 (R ): header0 (default: 0x00000000)\n",
            "    4 (R ): header1 (default: 0x00000000)\n",
            "    8 (R ): header2 (default: 0x00000000)\n",
            "   12 (R ): header3 (default: 0x00000000)\n",
            "   16 ( W): Start (set '1' to run)\n",
            "   20 (R ): Busy (returns '1' when running)\n",
            "   24 ( W): Reset (set '1' to initialize internal logic)\n",
            "   28 (R ): Opcode from extern objects to SW (returns '0' when idle)\n",
            "   32 ( W): Resume extern objects (set '1' to resume)\n",
            "   36 (R ): Interrupt Status Register\n",
            "   40 ( W): Interrupt Enable Register\n",
            "   44 ( W): Interrupt Acknowledge Register\n",
            "   48 (R ): State Counter\n",
            "   52 ( W): Count Target\n",
            "   56 ( W): Count Divider\n",
            "   60 (  ): Reserved ...\n",
            "  120 (  ): ... Reserved\n",
            "  124 (R ): Address space amount\n",
            "  128 (RW): Global address offset (default: 0)\n",
            "  132 (RW): Address of temporal storages (size: 0B)\n",
            "An error occurred during Veriloggen generation:\n",
            "math domain error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(param_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDLkz0HxLuxp",
        "outputId": "37259cc6-57e0-4b29-ee4d-8d7a1ec977a5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[235   3 202 ...   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from veriloggen import *\n",
        "import veriloggen.types.axi as axi\n",
        "import veriloggen.thread as vthread\n",
        "\n",
        "\n",
        "chunk_size = 64\n",
        "outputfile = 'complex.out'\n",
        "filename = 'complexcnn.v'\n",
        "\n",
        "param_bytes = len(param_data)\n",
        "\n",
        "variable_addr = int(\n",
        "    math.ceil((input_layer.addr + input_layer.memory_size) / chunk_size)) * chunk_size\n",
        "check_addr = int(math.ceil((variable_addr + param_bytes) / chunk_size)) * chunk_size\n",
        "tmp_addr = int(math.ceil((check_addr + output_layer.memory_size) / chunk_size)) * chunk_size\n",
        "\n",
        "memimg_datawidth = 32\n",
        "mem = np.zeros([1024 * 1024 * 256 // memimg_datawidth], dtype=np.int64)\n",
        "mem = mem + [100]\n",
        "\n",
        "# placeholder\n",
        "axi.set_memory(mem, input_layer_value, memimg_datawidth,\n",
        "               act_dtype.width, input_layer.addr,\n",
        "               max(int(math.ceil(axi_datawidth / act_dtype.width)), par_ich))\n",
        "\n",
        "# parameters (variable and constant)\n",
        "axi.set_memory(mem, param_data, memimg_datawidth,\n",
        "               8, variable_addr)\n",
        "\n",
        "# verification data\n",
        "axi.set_memory(mem, output_layer_value, memimg_datawidth,\n",
        "               act_dtype.width, check_addr,\n",
        "               max(int(math.ceil(axi_datawidth / act_dtype.width)), par_och))\n",
        "\n",
        "# test controller\n",
        "m = Module('test')\n",
        "params = m.copy_params(targ)\n",
        "ports = m.copy_sim_ports(targ)\n",
        "clk = ports['CLK']\n",
        "resetn = ports['RESETN']\n",
        "rst = m.Wire('RST')\n",
        "rst.assign(Not(resetn))\n",
        "\n",
        "# AXI memory model (skipped for brevity)\n",
        "\n",
        "# AXI-Slave controller (skipped for brevity)\n",
        "\n",
        "# timer (skipped for brevity)\n",
        "\n",
        "def ctrl():\n",
        "  pass\n",
        "    # Control logic (skipped for brevity)\n",
        "\n",
        "th = vthread.Thread(m, 'th_ctrl', clk, rst, ctrl)\n",
        "fsm = th.start()\n",
        "\n",
        "uut = m.Instance(targ, 'uut',\n",
        "                 params=m.connect_params(targ),\n",
        "                 ports=m.connect_ports(targ))\n",
        "\n",
        "# Output Verilog code\n",
        "if filename is not None:\n",
        "    m.to_verilog(filename)\n",
        "    print(\"verilog file generated successfully\")"
      ],
      "metadata": {
        "id": "Q4nTzMpB2tmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460a8b9f-6833-47b8-94cf-1483f89a74ed"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "verilog file generated successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the .npy file\n",
        "data = np.load('complexcnn.npy')\n",
        "\n",
        "# Display the data\n",
        "print(data)\n",
        "print(dir(output_layer))\n",
        "matmul_obj=output_layer\n",
        "print(\"Shape:\", matmul_obj.shape)\n",
        "print(\"Data type:\", matmul_obj.dtype)\n",
        "print(\"Arguments (input tensors):\", matmul_obj.value)\n",
        "\n",
        "import torch\n",
        "\n",
        "# Load the .pth file\n",
        "file_path = 'cifar_net.pth'\n",
        "data = torch.load(file_path)\n",
        "\n",
        "# Print the contents of the .pth file\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW_eEsUjxUuL",
        "outputId": "31baedcd-9df7-4617-a58c-388853e58706"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[235   3 202 ...   0   0   0]\n",
            "['Cparam', 'CparamWire', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__intrinsics__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__sub_str__', '__subclasshook__', '__weakref__', '_collect_arg_numerics', '_collect_numerics', '_conv2d__set_latency', '_name', 'act_bat_step', 'act_func', 'act_func_index', 'act_num_col', 'act_num_row', 'act_offset_values', 'act_read_block', 'act_read_size', 'act_read_step', 'act_row_step', 'add_alignment_request', 'add_consumer', 'addr', 'aligned_length', 'aligned_shape', 'aligned_size', 'arg_objaddrs', 'args', 'args_dict', 'asymmetric_clip', 'attribute', 'bias_num', 'bias_ram_size', 'bias_scala', 'cached_control', 'cached_ram_set', 'cached_stream', 'chain_head', 'check_ram_requirements', 'clk', 'col_select_initval', 'collect_all_control_param_names', 'collect_all_control_param_values', 'collect_arg_numerics', 'collect_local_control_param_names', 'collect_local_control_param_values', 'collect_numerics', 'collect_sources', 'concur_och', 'concur_och_value', 'concur_out_col', 'consumers', 'control', 'control_cachable', 'control_param_custom_signed', 'control_param_custom_width', 'control_param_index', 'control_param_index_reg', 'control_param_names', 'control_param_ram', 'control_sequence', 'copy_control', 'copy_control_params', 'copy_local_control_params', 'cshamt_mul', 'cshamt_mul_value', 'cshamt_out', 'cshamt_out_value', 'cshamt_sum', 'cshamt_sum_value', 'data_stationary', 'default_global_addr', 'default_local_addr', 'disable_keep_input', 'dma_flag_conds', 'dtype', 'eval', 'filter_base_step', 'filter_num_col_minus_stride_col_mod', 'filter_num_och', 'filter_ram_size', 'filter_read_block', 'filter_read_size', 'filter_read_step', 'filter_shape', 'get_aligned_length', 'get_aligned_shape', 'get_arg_default_global_addrs', 'get_arg_default_local_addrs', 'get_arg_global_indexes', 'get_arg_local_indexes', 'get_control_func', 'get_control_hash', 'get_control_param_values', 'get_default_alignment', 'get_eval_method', 'get_layout', 'get_length', 'get_local_control_param_values', 'get_max_arg_rank', 'get_min_concur_och', 'get_onnx_layout', 'get_op_point', 'get_op_width', 'get_original_layout', 'get_original_onnx_layout', 'get_original_shape', 'get_ram_set_obj_hash', 'get_ram_width', 'get_req_concur_och', 'get_required_rams', 'get_required_substreams', 'get_shared_attr_index', 'get_signed', 'get_stream_func', 'get_stream_hash', 'get_stream_obj_hash', 'get_total_control_param_width', 'get_word_alignment', 'global_index', 'has_bias', 'has_scale', 'has_vshamt_mul', 'has_vshamt_out', 'has_vshamt_sum', 'inc_act_laddr_conds', 'inc_act_laddr_large', 'inc_act_laddr_small', 'inc_out_laddr_col', 'inc_sync_out', 'inc_sync_out_res', 'input_chainable', 'input_ram_size', 'input_rams', 'input_shape', 'is_output', 'is_schedulable', 'is_scheduled', 'join_control', 'keep_filter', 'keep_filter_value', 'keep_input', 'keep_input_value', 'layout', 'left_ram_size', 'length', 'local_index', 'm', 'make_arg_objaddrs', 'make_control', 'make_control_param_buf', 'make_control_param_mux', 'make_control_param_ram', 'make_control_param_single', 'make_control_params', 'make_control_params_reg', 'make_control_params_wire', 'make_objaddr', 'make_stream', 'max_bat_count', 'max_col_count', 'max_och_count', 'max_row_count', 'maxi', 'memory_size', 'merge_shared_attrs', 'mul_dtype', 'name', 'objaddr', 'object_id', 'och_count_step', 'onnx_layout', 'orig_shape', 'out_bat_step', 'out_col_step', 'out_num_col', 'out_num_row', 'out_och_step', 'out_offset_values', 'out_ram_size', 'out_row_step', 'out_write_block', 'out_write_size', 'out_write_size_res', 'output_chainable', 'output_rams', 'pad_col_left', 'pad_col_left_value', 'pad_col_right_value', 'pad_row_bottom_value', 'pad_row_top', 'pad_row_top_value', 'padding', 'par', 'par_col', 'par_ich', 'par_left_col', 'par_left_row', 'par_och', 'par_out_col', 'par_row', 'parallel_scheduling_allowed', 'perm', 'quantized', 'reset_control', 'reversed_perm', 'right_ram_size', 'rst', 'run_control', 'saxi', 'scale_factor', 'scale_num', 'scale_ram_size', 'scale_scala', 'set_control_param_index', 'set_control_params', 'set_control_params_mux', 'set_control_params_ram', 'set_default_global_addr', 'set_default_local_addr', 'set_global_index', 'set_local_index', 'set_module_info', 'set_output', 'set_rams', 'set_stage', 'set_stream', 'set_substreams', 'set_value', 'shape', 'shared_attr_names', 'shared_attrs', 'size', 'stage', 'stationary', 'stream', 'stream_act_local_large_flags', 'stream_act_local_large_offset', 'stream_act_local_small_flags', 'stream_act_local_small_offset', 'stream_aligned_reduce_size', 'stream_cachable', 'stream_num_ops', 'stream_num_ops_par', 'stream_num_ops_res', 'stream_num_ops_res_par', 'stream_omit_mask', 'stream_reduce_size', 'stride_bat', 'stride_col_mod_filter_num', 'stride_col_par_col', 'stride_row_par_row', 'strides', 'substreams', 'sum_dtype', 'temp_rams', 'to_local_control_param_name', 'transposed_a', 'transposed_b', 'value', 'vshamt_mul_num', 'vshamt_mul_ram_size', 'vshamt_mul_scala', 'vshamt_out_num', 'vshamt_out_ram_size', 'vshamt_out_scala', 'vshamt_sum_num', 'vshamt_sum_ram_size', 'vshamt_sum_scala', 'word_alignment']\n",
            "Shape: (1, 10)\n",
            "Data type: <dtype int16>\n",
            "Arguments (input tensors): None\n",
            "OrderedDict([('conv1.weight', tensor([[[[ 0.1532,  0.2262,  0.1518],\n",
            "          [-0.1939, -0.2444, -0.0565],\n",
            "          [-0.1329, -0.2885, -0.1596]],\n",
            "\n",
            "         [[-0.0299,  0.2585,  0.1343],\n",
            "          [-0.1591,  0.1495,  0.2431],\n",
            "          [-0.1123, -0.0622, -0.0253]],\n",
            "\n",
            "         [[-0.0038, -0.0171,  0.0136],\n",
            "          [ 0.0598,  0.1440, -0.1341],\n",
            "          [ 0.1565, -0.1545,  0.0560]]],\n",
            "\n",
            "\n",
            "        [[[-0.0560, -0.2677, -0.0152],\n",
            "          [-0.3005, -0.2657, -0.2643],\n",
            "          [-0.1437, -0.1792, -0.2201]],\n",
            "\n",
            "         [[ 0.1807,  0.3710,  0.0316],\n",
            "          [ 0.4806,  0.6360,  0.4813],\n",
            "          [ 0.2348,  0.3678,  0.3855]],\n",
            "\n",
            "         [[-0.0407, -0.2538,  0.0169],\n",
            "          [-0.3217, -0.2495, -0.3543],\n",
            "          [-0.0284, -0.4016, -0.1158]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0054,  0.0565,  0.0595],\n",
            "          [ 0.1971, -0.0149, -0.2864],\n",
            "          [-0.0724,  0.1028, -0.0829]],\n",
            "\n",
            "         [[ 0.1058,  0.2043, -0.2594],\n",
            "          [ 0.2741, -0.0642, -0.1653],\n",
            "          [ 0.1833, -0.0568, -0.2118]],\n",
            "\n",
            "         [[ 0.2582, -0.0627, -0.1724],\n",
            "          [ 0.0827,  0.0415, -0.1085],\n",
            "          [ 0.2359, -0.1181, -0.2380]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.3017,  0.2462,  0.1065],\n",
            "          [ 0.3258, -0.2903, -0.2470],\n",
            "          [-0.0052, -0.3644, -0.3301]],\n",
            "\n",
            "         [[ 0.0238, -0.2379, -0.0451],\n",
            "          [-0.0269, -0.1607,  0.2029],\n",
            "          [ 0.1282,  0.0504,  0.1970]],\n",
            "\n",
            "         [[-0.1761, -0.1161, -0.0581],\n",
            "          [-0.1644, -0.0028,  0.1290],\n",
            "          [-0.0969,  0.0391,  0.3500]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1231, -0.1953, -0.1040],\n",
            "          [ 0.1350,  0.1959, -0.2623],\n",
            "          [-0.0322,  0.1045,  0.1441]],\n",
            "\n",
            "         [[-0.0161, -0.1821,  0.0909],\n",
            "          [ 0.1384,  0.1617, -0.2822],\n",
            "          [-0.1715,  0.2259,  0.0616]],\n",
            "\n",
            "         [[-0.1019, -0.0985, -0.0917],\n",
            "          [ 0.2044, -0.0564, -0.3139],\n",
            "          [ 0.0398,  0.3717, -0.0666]]],\n",
            "\n",
            "\n",
            "        [[[-0.1340,  0.1684,  0.1261],\n",
            "          [ 0.0301,  0.1927,  0.0225],\n",
            "          [ 0.1631, -0.1491, -0.2873]],\n",
            "\n",
            "         [[ 0.1753,  0.1475,  0.1032],\n",
            "          [ 0.0970, -0.0266,  0.0565],\n",
            "          [ 0.1466, -0.1547, -0.1612]],\n",
            "\n",
            "         [[ 0.1148, -0.2044, -0.0840],\n",
            "          [-0.2260,  0.0216, -0.1739],\n",
            "          [-0.0629, -0.1477, -0.2780]]]])), ('conv1.bias', tensor([ 0.0933,  0.0740,  0.0428, -0.0508, -0.0494,  0.1101, -0.1711,  0.1821,\n",
            "        -0.0298, -0.0801, -0.0764, -0.1889, -0.1527, -0.1819, -0.0856, -0.1428,\n",
            "        -0.1825,  0.1002, -0.0570,  0.0708,  0.1269,  0.1354, -0.0635,  0.1301,\n",
            "         0.1555,  0.1867, -0.0967,  0.1013,  0.0264,  0.0206,  0.0644, -0.0647,\n",
            "         0.0644, -0.1534,  0.1312,  0.0328,  0.0543,  0.0149,  0.0810,  0.1336,\n",
            "         0.1763, -0.0136, -0.0426, -0.1895,  0.0610,  0.0825,  0.0085,  0.0837,\n",
            "         0.0939,  0.1764, -0.1634, -0.1226,  0.1793, -0.0947, -0.0985, -0.0057,\n",
            "        -0.0871,  0.0604, -0.1642, -0.1744, -0.0022,  0.1426, -0.0156, -0.0958])), ('bn1.weight', tensor([0.9467, 1.0513, 1.0188, 1.1585, 0.9601, 0.8378, 0.9879, 0.9800, 1.1063,\n",
            "        1.0106, 1.0956, 1.0021, 1.1336, 1.1265, 0.9774, 1.0333, 0.9497, 1.2447,\n",
            "        0.8762, 0.8764, 0.9508, 0.8587, 0.9090, 1.0686, 1.2405, 0.8065, 1.1414,\n",
            "        0.7919, 1.0399, 1.0175, 0.9065, 0.8901, 0.8814, 0.9021, 0.9373, 0.9909,\n",
            "        1.0049, 0.9625, 1.1685, 1.0791, 0.9430, 1.0550, 0.7762, 0.8672, 1.0936,\n",
            "        0.9392, 0.9171, 1.1706, 1.1679, 0.9464, 0.9171, 0.8981, 1.0470, 0.9503,\n",
            "        0.9508, 0.8404, 1.0934, 1.0027, 0.9799, 0.9752, 0.8922, 0.9905, 1.0260,\n",
            "        1.0384])), ('bn1.bias', tensor([-0.0388,  0.0880, -0.1026, -0.1343, -0.2389, -0.1039, -0.0544, -0.0938,\n",
            "        -0.0359, -0.0297, -0.0592, -0.1165, -0.1009, -0.0413,  0.0402, -0.0415,\n",
            "        -0.2245,  0.0198, -0.0103, -0.2341,  0.0144, -0.2272, -0.2575,  0.0352,\n",
            "        -0.1464, -0.0196,  0.0216, -0.0036, -0.0005, -0.0174, -0.1274, -0.1862,\n",
            "        -0.0803, -0.1669, -0.0181, -0.3091, -0.0366, -0.0400, -0.0559, -0.1060,\n",
            "        -0.1355, -0.0032,  0.1381, -0.1996, -0.0108, -0.0664,  0.0015, -0.0556,\n",
            "         0.0137, -0.2155, -0.2047, -0.1808, -0.0825,  0.0066, -0.0473, -0.1730,\n",
            "         0.0947, -0.0543, -0.1997,  0.1404, -0.2606,  0.0100, -0.0409, -0.0923])), ('bn1.running_mean', tensor([ 0.1079,  0.0645,  0.0440, -0.0338, -0.0636,  0.1595, -0.1918,  0.1870,\n",
            "        -0.0313, -0.0861, -0.0721, -0.1839, -0.1500, -0.1795, -0.0778, -0.1374,\n",
            "        -0.1881,  0.1079, -0.0607,  0.0737,  0.1229,  0.1341, -0.0500,  0.1364,\n",
            "         0.1567,  0.2068, -0.1009,  0.1349,  0.0115,  0.0091,  0.0561, -0.0507,\n",
            "         0.0751, -0.1606,  0.1466, -0.0019,  0.0543,  0.0072,  0.0800,  0.1261,\n",
            "         0.1700, -0.0194, -0.0014, -0.1971,  0.0507,  0.0850,  0.0042,  0.0840,\n",
            "         0.0883,  0.1793, -0.1858, -0.1371,  0.1939, -0.0931, -0.0860,  0.0271,\n",
            "        -0.0838,  0.0580, -0.1676, -0.1789,  0.0084,  0.1376, -0.0246, -0.1127])), ('bn1.running_var', tensor([0.5001, 1.0494, 0.8797, 0.6511, 0.1665, 1.4383, 0.7887, 0.6293, 0.7993,\n",
            "        0.4809, 0.9576, 0.2729, 0.2466, 0.5578, 0.9798, 0.4662, 0.4748, 1.0961,\n",
            "        0.1632, 0.7714, 0.8503, 0.3169, 0.6363, 0.5831, 1.0296, 0.2802, 0.5326,\n",
            "        1.0868, 0.2493, 1.0228, 0.4154, 0.3673, 1.6535, 1.8344, 0.4422, 0.8468,\n",
            "        0.5188, 0.6344, 1.4025, 0.3990, 0.6051, 0.9087, 0.9330, 0.7459, 0.8978,\n",
            "        0.2429, 0.3977, 0.4938, 0.2262, 0.4496, 1.7840, 0.7761, 0.9347, 0.3877,\n",
            "        0.5383, 0.9341, 0.3906, 0.4989, 0.4816, 0.9149, 0.6783, 0.2255, 0.4820,\n",
            "        0.9830])), ('bn1.num_batches_tracked', tensor(5100)), ('conv2.weight', tensor([[[[-4.0651e-02, -2.4470e-02, -1.1485e-02],\n",
            "          [ 1.3072e-02, -2.9182e-02, -7.9453e-03],\n",
            "          [-9.2399e-03,  2.6526e-02,  4.5806e-02]],\n",
            "\n",
            "         [[ 1.2679e-02,  1.0307e-01,  8.5777e-02],\n",
            "          [-5.0117e-02, -6.7858e-02,  1.9639e-02],\n",
            "          [-5.9596e-02, -1.0344e-01, -2.2390e-02]],\n",
            "\n",
            "         [[ 1.7120e-02, -2.6428e-02,  8.1993e-03],\n",
            "          [ 2.6887e-02,  3.7655e-02,  4.3002e-02],\n",
            "          [ 3.6245e-02,  1.7747e-02,  1.0424e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.1967e-02, -2.0121e-03,  7.2186e-02],\n",
            "          [ 5.4588e-02,  4.2294e-02, -5.4717e-03],\n",
            "          [ 1.7417e-02, -2.5597e-02, -2.0023e-02]],\n",
            "\n",
            "         [[ 1.4833e-03,  2.2165e-02,  1.7146e-02],\n",
            "          [-3.6887e-02, -1.8767e-02, -1.4765e-02],\n",
            "          [ 2.4033e-03,  3.8822e-02,  1.1914e-02]],\n",
            "\n",
            "         [[ 4.4027e-02,  1.7043e-02,  5.3741e-03],\n",
            "          [-7.4350e-02, -8.8006e-02, -2.9505e-02],\n",
            "          [-5.8732e-02, -5.2083e-02,  2.0230e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4174e-02,  4.3155e-03,  2.7452e-02],\n",
            "          [-5.3543e-03, -4.2589e-02,  8.3775e-03],\n",
            "          [ 1.1968e-02,  1.8758e-02,  1.3462e-02]],\n",
            "\n",
            "         [[-7.5869e-03, -4.6278e-02,  3.2174e-02],\n",
            "          [ 6.3809e-03, -7.9278e-02,  3.1481e-02],\n",
            "          [ 4.9223e-02, -6.0985e-02,  4.0085e-02]],\n",
            "\n",
            "         [[ 1.6333e-02, -2.1790e-02, -6.5632e-02],\n",
            "          [ 3.4340e-02, -4.5202e-02, -3.0894e-02],\n",
            "          [-2.2224e-02, -2.0481e-02, -2.1254e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.8609e-02,  1.2912e-02,  3.3672e-02],\n",
            "          [-1.6432e-02,  5.4091e-02,  1.0809e-02],\n",
            "          [-2.9856e-02,  7.6543e-02, -2.6364e-02]],\n",
            "\n",
            "         [[-2.7136e-02, -4.7195e-02,  6.9975e-03],\n",
            "          [-1.6124e-02, -4.6859e-02, -6.8732e-03],\n",
            "          [-5.8880e-03,  3.7942e-02, -1.8508e-02]],\n",
            "\n",
            "         [[-6.8231e-03, -2.6770e-02,  3.4783e-02],\n",
            "          [ 3.2525e-02,  4.8981e-03,  5.0183e-02],\n",
            "          [ 6.1830e-02,  4.2338e-02,  4.6787e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4964e-02,  1.2333e-01,  4.2527e-02],\n",
            "          [-3.9555e-03,  2.1387e-02,  2.5298e-02],\n",
            "          [ 6.6020e-03, -9.3790e-03,  5.0887e-02]],\n",
            "\n",
            "         [[ 3.7324e-02,  1.4422e-02, -2.6193e-03],\n",
            "          [-5.9031e-02, -1.1205e-01, -1.3891e-02],\n",
            "          [-9.2925e-02, -9.2362e-02, -8.4563e-02]],\n",
            "\n",
            "         [[-8.8330e-03, -9.1295e-02, -3.7459e-02],\n",
            "          [-3.6376e-04,  9.1091e-03, -4.6089e-02],\n",
            "          [ 1.4715e-02, -9.3531e-03,  5.1968e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.1226e-02, -2.1184e-02, -2.4260e-02],\n",
            "          [ 6.8843e-02,  8.6579e-03, -1.5773e-02],\n",
            "          [-1.4351e-02, -1.2510e-02,  2.4603e-02]],\n",
            "\n",
            "         [[-2.1521e-02, -4.3131e-03,  3.9927e-02],\n",
            "          [ 7.1801e-04, -2.4856e-02,  3.1849e-02],\n",
            "          [ 5.5375e-02,  1.6350e-02, -5.8906e-02]],\n",
            "\n",
            "         [[ 2.8390e-02,  8.6220e-03,  1.4414e-02],\n",
            "          [-3.6966e-02, -9.7665e-02, -8.8910e-03],\n",
            "          [-1.9115e-02, -6.0337e-02, -2.4602e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 4.7383e-04, -2.6736e-02,  2.8682e-02],\n",
            "          [-4.2450e-02,  3.4309e-03, -1.5109e-02],\n",
            "          [ 1.6255e-02, -3.2986e-02,  1.3197e-02]],\n",
            "\n",
            "         [[-1.1607e-01, -3.4907e-02,  6.7573e-02],\n",
            "          [-6.7352e-02, -9.8978e-03,  8.0115e-02],\n",
            "          [ 1.2025e-02,  9.6108e-04,  7.8876e-02]],\n",
            "\n",
            "         [[ 3.8451e-02, -4.9453e-02,  1.7004e-02],\n",
            "          [-1.3488e-02, -3.9503e-03, -2.3178e-02],\n",
            "          [-1.8102e-04, -1.0999e-02,  1.1039e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.1123e-02,  3.3876e-03, -5.5193e-02],\n",
            "          [ 5.2572e-02, -3.6409e-02, -2.7959e-02],\n",
            "          [-2.7461e-02, -4.9897e-02,  2.0061e-02]],\n",
            "\n",
            "         [[-9.2627e-02,  6.3539e-02, -2.6593e-02],\n",
            "          [-2.5811e-02,  5.8963e-02, -7.1510e-02],\n",
            "          [ 6.6066e-02, -2.8746e-03, -2.9654e-02]],\n",
            "\n",
            "         [[ 1.7910e-02, -1.0232e-01, -3.3907e-02],\n",
            "          [ 2.3037e-03,  2.1101e-02,  1.0860e-04],\n",
            "          [-7.7963e-03,  3.4173e-02,  1.9500e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.9127e-02,  7.3904e-03, -1.8871e-02],\n",
            "          [-6.2480e-03, -4.5156e-02, -5.7889e-02],\n",
            "          [-6.5294e-02, -5.8097e-02, -1.1459e-02]],\n",
            "\n",
            "         [[ 2.8668e-03, -7.6269e-03, -2.9474e-02],\n",
            "          [-4.3609e-02, -6.5526e-02, -7.8388e-02],\n",
            "          [-4.8875e-02, -5.1664e-02, -2.2635e-02]],\n",
            "\n",
            "         [[-1.9363e-03,  2.3595e-02,  5.5133e-02],\n",
            "          [ 5.4391e-02,  4.8974e-02,  7.6901e-03],\n",
            "          [ 3.2155e-03,  2.4854e-02,  4.5191e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7717e-02, -1.6405e-02,  3.3350e-02],\n",
            "          [-4.0021e-02,  7.3493e-03,  1.5939e-02],\n",
            "          [-4.0612e-04, -6.6383e-04, -5.2255e-03]],\n",
            "\n",
            "         [[ 4.7864e-02,  2.8927e-02, -1.1633e-03],\n",
            "          [-2.9743e-02, -3.7254e-02, -3.5107e-02],\n",
            "          [-1.8876e-02,  2.1955e-02,  3.9075e-02]],\n",
            "\n",
            "         [[-1.9286e-02, -7.1456e-03,  3.6255e-02],\n",
            "          [-4.2145e-02,  2.2160e-02,  7.0113e-03],\n",
            "          [-7.9427e-02, -6.1677e-02, -6.2344e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.0205e-02, -2.3498e-02,  1.4196e-02],\n",
            "          [ 5.3369e-03, -7.4289e-05, -6.5851e-03],\n",
            "          [ 2.8512e-02,  7.8920e-02,  1.8009e-02]],\n",
            "\n",
            "         [[-1.0774e-01, -1.1033e-01, -7.0797e-02],\n",
            "          [ 1.9243e-02, -8.1254e-03,  1.7706e-02],\n",
            "          [ 1.1645e-01,  1.5353e-01,  1.0827e-01]],\n",
            "\n",
            "         [[-2.5730e-02,  8.3504e-03,  8.4165e-03],\n",
            "          [-1.3629e-03, -1.6058e-02, -1.9505e-02],\n",
            "          [-1.0030e-01, -9.3626e-02, -1.0695e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2889e-02, -1.5862e-02, -2.5936e-02],\n",
            "          [-5.4724e-02,  1.1983e-03, -4.3311e-02],\n",
            "          [ 1.5783e-02, -8.5597e-03, -2.8347e-02]],\n",
            "\n",
            "         [[ 2.3205e-02, -1.8653e-03, -2.6231e-02],\n",
            "          [ 4.2133e-02,  1.5103e-02,  8.6894e-03],\n",
            "          [ 8.0215e-03,  4.0410e-02,  3.9687e-02]],\n",
            "\n",
            "         [[-1.3756e-02, -5.1843e-02, -1.7923e-02],\n",
            "          [-1.4185e-02,  6.0515e-03,  3.8190e-02],\n",
            "          [-1.6701e-02, -1.0463e-02, -2.8779e-03]]]])), ('conv2.bias', tensor([-0.0314, -0.0315,  0.0054, -0.0144,  0.0025, -0.0306,  0.0144,  0.0006,\n",
            "         0.0273,  0.0113,  0.0139,  0.0201, -0.0279,  0.0360, -0.0247,  0.0329,\n",
            "        -0.0004, -0.0247,  0.0297, -0.0165, -0.0163, -0.0217,  0.0258, -0.0404,\n",
            "         0.0307,  0.0234,  0.0290,  0.0210, -0.0219, -0.0043, -0.0275,  0.0110,\n",
            "        -0.0158,  0.0004,  0.0068,  0.0013,  0.0113,  0.0324, -0.0370,  0.0303,\n",
            "         0.0054, -0.0383, -0.0045,  0.0047,  0.0179, -0.0380,  0.0280, -0.0335,\n",
            "        -0.0029,  0.0344, -0.0112, -0.0078, -0.0369, -0.0180,  0.0332,  0.0094,\n",
            "        -0.0089,  0.0178, -0.0377,  0.0391, -0.0139,  0.0263, -0.0286,  0.0317,\n",
            "         0.0160, -0.0406,  0.0290, -0.0090,  0.0224, -0.0347,  0.0255,  0.0079,\n",
            "        -0.0011,  0.0186, -0.0011, -0.0311,  0.0011,  0.0190,  0.0302, -0.0018,\n",
            "        -0.0022, -0.0004,  0.0139, -0.0224, -0.0037,  0.0099,  0.0073,  0.0339,\n",
            "         0.0128, -0.0304, -0.0033, -0.0384, -0.0016, -0.0051,  0.0220, -0.0266,\n",
            "         0.0215,  0.0208, -0.0279,  0.0014,  0.0374,  0.0313,  0.0303, -0.0042,\n",
            "         0.0318, -0.0130, -0.0285, -0.0325, -0.0311, -0.0346, -0.0339, -0.0396,\n",
            "         0.0338,  0.0154,  0.0410,  0.0331,  0.0095, -0.0236, -0.0206, -0.0084,\n",
            "         0.0309,  0.0128,  0.0311,  0.0236, -0.0360, -0.0002,  0.0344,  0.0217])), ('bn2.weight', tensor([0.9961, 0.9156, 0.9576, 1.1241, 0.9157, 0.9780, 1.0616, 0.9431, 0.9421,\n",
            "        0.9816, 0.9684, 0.9872, 1.0609, 0.9551, 1.0597, 0.9606, 0.9929, 1.0639,\n",
            "        1.0327, 0.9052, 0.9624, 0.9382, 0.9768, 0.9751, 1.0156, 1.1647, 1.1107,\n",
            "        0.9738, 0.9972, 0.9275, 0.9307, 0.9816, 0.9916, 0.9626, 1.0211, 0.9660,\n",
            "        0.8629, 0.9317, 0.9121, 0.9290, 1.0717, 0.9948, 1.0659, 1.0010, 1.0986,\n",
            "        1.0352, 0.8843, 0.9496, 0.9378, 1.0615, 1.0151, 1.0275, 1.0359, 1.0032,\n",
            "        1.0272, 0.9328, 1.0671, 0.9887, 0.9435, 0.8879, 0.8505, 1.0246, 0.9715,\n",
            "        0.9937, 0.9276, 0.9949, 1.0076, 0.9939, 0.9231, 1.0598, 1.0124, 0.9551,\n",
            "        1.0209, 0.9175, 0.9885, 1.0788, 0.9322, 0.9997, 0.9396, 0.9212, 0.9005,\n",
            "        1.0320, 1.0189, 1.0829, 0.9325, 0.9936, 1.0190, 0.9467, 0.9748, 1.0793,\n",
            "        0.9411, 0.9397, 0.8766, 1.0537, 1.0179, 0.8959, 1.0855, 1.0374, 1.1738,\n",
            "        0.9822, 0.9690, 0.9652, 0.9512, 0.9653, 0.9746, 0.9156, 0.9687, 0.9322,\n",
            "        0.9333, 0.9872, 0.9413, 0.9203, 0.9417, 1.0114, 0.9152, 0.9305, 1.0342,\n",
            "        1.1019, 0.9528, 0.9967, 1.1094, 1.0532, 0.9732, 1.0247, 0.9756, 0.9913,\n",
            "        1.0027, 1.0393])), ('bn2.bias', tensor([-0.1794, -0.1333, -0.2359, -0.1723, -0.1679, -0.1793, -0.1519, -0.1494,\n",
            "        -0.0782, -0.1324, -0.1244, -0.1547, -0.1967, -0.1874, -0.1933, -0.1839,\n",
            "        -0.1414, -0.1416, -0.1322, -0.1586, -0.1153, -0.1355, -0.0948, -0.1850,\n",
            "        -0.0923, -0.1460, -0.3156, -0.1240, -0.1688, -0.1144, -0.0554, -0.1392,\n",
            "        -0.1682, -0.1467, -0.1147, -0.1211, -0.1323, -0.1945, -0.1627, -0.1321,\n",
            "        -0.1357, -0.1142, -0.1492, -0.1753, -0.2051, -0.2068, -0.1456, -0.1418,\n",
            "        -0.1245, -0.2265, -0.1746, -0.0876, -0.1827, -0.0907, -0.1550, -0.1087,\n",
            "        -0.1339, -0.1283, -0.1399, -0.1159, -0.1543, -0.0917, -0.1170, -0.1197,\n",
            "        -0.1657, -0.2258, -0.2103, -0.1598, -0.1924, -0.1729, -0.0984, -0.1190,\n",
            "        -0.1366, -0.2502, -0.1922, -0.2232, -0.0861, -0.1051, -0.2437, -0.1065,\n",
            "        -0.1268, -0.1396, -0.0622, -0.2066, -0.1134, -0.1583, -0.2288, -0.2120,\n",
            "        -0.0663, -0.1958, -0.1572, -0.1881, -0.1700, -0.1102, -0.1099, -0.1272,\n",
            "        -0.2194, -0.1109, -0.3141, -0.1724, -0.1634, -0.1926, -0.2054, -0.1437,\n",
            "        -0.1533, -0.1546, -0.0828, -0.1481, -0.1187, -0.1011, -0.1909, -0.1156,\n",
            "        -0.1226, -0.1037, -0.1357, -0.1237, -0.1605, -0.1119, -0.1557, -0.1114,\n",
            "        -0.2158, -0.1949, -0.1032, -0.0487, -0.1719, -0.1786, -0.0835, -0.1754])), ('bn2.running_mean', tensor([-1.6755, -0.2631, -0.2376, -3.2859,  0.1957, -0.0459, -1.6956,  2.8071,\n",
            "        -0.6945, -1.6642, -1.4862, -2.0517, -1.9615, -0.2696, -1.2532,  2.3292,\n",
            "        -0.9581, -1.6026, -1.4472, -0.1977, -1.7393, -1.1659, -0.8412,  0.5237,\n",
            "        -1.3400, -1.5667, -0.5421, -0.9378, -1.4823, -0.6869, -0.8138, -1.0221,\n",
            "        -1.4184, -2.0645, -1.5485, -0.8360,  0.2905, -1.9371,  0.1775,  0.2943,\n",
            "        -1.4786, -2.4530, -1.3507, -1.4578, -1.0160, -0.8452, -0.9854, -1.7888,\n",
            "        -1.7168, -0.4222, -1.4659, -1.3887, -2.2804, -0.7531, -2.6335, -0.9460,\n",
            "        -1.3756, -1.3911, -1.2509, -1.0273,  0.0663, -1.6794, -0.4855, -1.0163,\n",
            "        -0.5358, -0.5589, -1.5869, -1.2247, -0.0315, -1.4731, -1.0543, -0.2111,\n",
            "        -1.4015, -1.3673, -0.8815, -2.2594, -0.9582, -1.1557, -1.1500,  1.2224,\n",
            "        -0.0583, -2.5527, -0.9392, -2.8373, -1.7442, -0.0613,  0.3361, -0.5101,\n",
            "        -0.6625, -2.5869, -1.0592,  0.0974,  1.1349, -2.6810, -1.3332, -0.6359,\n",
            "        -1.2574, -2.1877, -0.8033, -0.7227, -1.4529, -1.8894, -0.5731, -1.1100,\n",
            "        -1.0626, -0.5056, -1.2162, -1.1085, -1.7542, -1.5746, -1.1850,  0.0116,\n",
            "        -1.0980, -1.7401, -0.5043, -1.2922, -1.5494, -0.9473, -1.2592, -0.6532,\n",
            "        -2.8069, -1.9882, -1.3970,  0.1161, -1.4786, -0.8511, -2.1660, -1.3029])), ('bn2.running_var', tensor([1.4390, 1.4467, 1.4678, 4.5106, 1.4801, 1.8614, 2.1943, 2.9427, 1.5499,\n",
            "        1.7670, 1.1494, 2.3403, 2.3000, 1.8168, 2.0992, 2.1788, 1.5291, 2.4200,\n",
            "        2.2898, 1.7622, 1.7625, 2.0035, 1.3019, 1.8147, 1.7377, 2.1699, 3.5820,\n",
            "        2.0908, 2.3202, 1.4790, 1.7986, 1.3663, 1.4132, 1.8706, 1.7935, 1.1766,\n",
            "        1.4746, 1.9468, 1.9348, 1.3760, 1.8214, 2.4659, 1.9638, 1.2189, 2.2789,\n",
            "        2.1498, 1.7537, 1.5884, 1.9796, 1.3340, 1.5623, 1.8876, 2.1680, 1.8265,\n",
            "        3.1076, 0.9658, 1.8490, 1.3284, 2.7681, 1.5133, 1.3934, 2.0373, 1.2241,\n",
            "        1.2270, 1.3149, 1.2934, 2.2377, 1.6638, 1.5229, 2.0419, 1.2506, 1.7858,\n",
            "        1.8241, 1.9623, 2.1823, 3.1333, 1.6880, 1.3752, 2.6168, 1.8222, 1.6565,\n",
            "        3.2754, 1.6221, 2.9352, 1.1712, 2.2421, 3.0716, 1.3143, 1.1634, 3.3579,\n",
            "        1.1602, 1.6207, 2.4689, 2.7192, 1.4938, 1.2698, 2.0532, 2.9316, 3.8433,\n",
            "        1.9855, 1.9031, 1.4301, 1.8799, 1.8876, 1.8918, 1.1459, 1.4242, 1.5629,\n",
            "        1.6765, 1.4699, 1.9561, 1.6169, 1.4697, 2.1655, 1.7872, 1.6235, 1.9819,\n",
            "        1.7025, 1.4339, 1.0480, 5.3233, 3.6034, 1.4320, 2.8665, 1.5448, 1.8237,\n",
            "        1.8821, 1.6469])), ('bn2.num_batches_tracked', tensor(5100)), ('conv3.weight', tensor([[[[ 1.5454e-02, -4.7840e-02, -7.4507e-03],\n",
            "          [-3.9090e-02, -1.5147e-02,  1.1323e-02],\n",
            "          [-3.6398e-03,  1.7070e-02, -1.1929e-02]],\n",
            "\n",
            "         [[ 1.5864e-02, -1.4634e-02,  2.4032e-02],\n",
            "          [-9.4033e-03,  3.8208e-03,  5.5331e-03],\n",
            "          [-4.7673e-03,  1.2032e-03,  2.9118e-02]],\n",
            "\n",
            "         [[-1.3337e-03,  6.9978e-03,  1.9790e-02],\n",
            "          [ 2.7266e-02, -2.5598e-02,  1.7734e-02],\n",
            "          [-2.9448e-02,  2.0676e-02,  6.2755e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6676e-02, -7.0444e-03, -1.0575e-03],\n",
            "          [ 2.6302e-02, -1.3716e-02,  1.3011e-02],\n",
            "          [ 2.0072e-03,  1.7410e-02, -1.4889e-02]],\n",
            "\n",
            "         [[ 2.2288e-02,  2.5913e-02, -3.9731e-02],\n",
            "          [ 2.2893e-02,  5.6227e-02, -3.1806e-02],\n",
            "          [ 5.9697e-03,  7.0090e-03, -3.1337e-02]],\n",
            "\n",
            "         [[ 1.0753e-02, -4.3030e-02,  1.1620e-06],\n",
            "          [ 2.7617e-02, -2.6116e-02, -2.7685e-02],\n",
            "          [ 7.2505e-03, -3.1891e-02, -3.2626e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.1138e-02,  2.6704e-02, -8.7580e-05],\n",
            "          [-4.8859e-02,  4.0186e-02,  2.7459e-02],\n",
            "          [-2.0038e-02, -3.5901e-03,  3.2784e-02]],\n",
            "\n",
            "         [[-1.9782e-04,  7.6954e-03, -7.5272e-03],\n",
            "          [-2.7422e-02, -3.5720e-02, -5.2586e-03],\n",
            "          [ 6.1144e-03,  9.5296e-03,  8.5040e-03]],\n",
            "\n",
            "         [[-6.3051e-03,  1.4487e-02,  9.5297e-03],\n",
            "          [-5.1158e-03,  2.3746e-02,  2.9239e-02],\n",
            "          [-2.1045e-03, -9.6766e-03, -1.7054e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2284e-03, -4.9036e-03,  1.6170e-02],\n",
            "          [-4.5639e-03, -3.6166e-03, -7.3351e-04],\n",
            "          [ 8.9542e-03, -3.3292e-02, -2.6032e-03]],\n",
            "\n",
            "         [[-3.3019e-03, -3.9585e-03, -2.4152e-02],\n",
            "          [ 2.7523e-02, -3.1273e-02, -5.4480e-03],\n",
            "          [ 1.1240e-02,  2.0813e-02,  3.5717e-03]],\n",
            "\n",
            "         [[-2.0678e-02, -5.0672e-03,  6.1664e-03],\n",
            "          [-1.1724e-02, -2.7927e-02,  5.6961e-03],\n",
            "          [-2.9880e-02,  4.4371e-03,  5.8568e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 6.7868e-04,  2.5943e-02, -2.4206e-02],\n",
            "          [-3.0684e-02, -1.0200e-02, -1.3352e-02],\n",
            "          [-3.3243e-02, -3.0669e-02,  1.6020e-03]],\n",
            "\n",
            "         [[ 3.6639e-02, -2.6088e-03,  1.6924e-02],\n",
            "          [ 1.5537e-02, -2.9867e-02,  2.6844e-03],\n",
            "          [-2.3805e-02,  1.1350e-03,  1.3671e-02]],\n",
            "\n",
            "         [[-5.2798e-03,  2.1790e-02, -1.1752e-02],\n",
            "          [-1.6626e-02, -2.2123e-02,  1.6287e-04],\n",
            "          [-3.8606e-03, -3.9363e-02, -1.3732e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.1208e-03, -1.0843e-02, -2.4159e-02],\n",
            "          [-4.3322e-04,  5.5138e-03,  1.3633e-02],\n",
            "          [-1.3619e-03,  1.0200e-02, -2.2560e-02]],\n",
            "\n",
            "         [[ 7.3147e-03, -3.0083e-02,  2.1819e-02],\n",
            "          [ 1.7146e-02,  1.1580e-02,  5.8488e-03],\n",
            "          [-1.3799e-02,  4.4708e-04, -9.8666e-03]],\n",
            "\n",
            "         [[ 1.5935e-02,  1.4078e-02,  2.2661e-02],\n",
            "          [ 3.9770e-02,  4.0989e-02,  3.9468e-02],\n",
            "          [ 3.2682e-02,  4.5031e-02,  1.3788e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.6003e-02, -2.5434e-03, -6.8508e-03],\n",
            "          [ 3.7972e-02,  2.1143e-02,  1.2294e-02],\n",
            "          [ 3.1144e-02,  1.5521e-02, -2.2925e-02]],\n",
            "\n",
            "         [[ 1.8226e-02, -1.6666e-02,  3.4423e-02],\n",
            "          [-1.6716e-02, -2.5484e-02, -1.5883e-02],\n",
            "          [ 1.0656e-02,  7.9681e-03,  2.1462e-02]],\n",
            "\n",
            "         [[-1.1453e-02,  8.5563e-03,  5.8757e-03],\n",
            "          [-1.7744e-02, -4.4873e-03, -6.6149e-03],\n",
            "          [ 7.4677e-04,  1.1847e-02,  6.3630e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0018e-02,  1.1254e-02, -1.0223e-02],\n",
            "          [ 3.5337e-02,  4.3858e-03, -2.0524e-02],\n",
            "          [-3.6989e-04,  2.4111e-02, -3.9781e-02]],\n",
            "\n",
            "         [[-1.2539e-02, -9.4658e-03,  1.2675e-02],\n",
            "          [-6.6334e-03,  1.1674e-02,  2.4229e-02],\n",
            "          [ 7.8714e-03, -1.6957e-02,  1.0845e-02]],\n",
            "\n",
            "         [[ 2.6343e-02,  1.8291e-03,  2.4229e-02],\n",
            "          [-1.3849e-02, -3.4519e-03, -6.2020e-04],\n",
            "          [ 3.2173e-03, -3.0518e-02, -3.2259e-02]]],\n",
            "\n",
            "\n",
            "        [[[-6.5527e-03, -1.4304e-02, -1.4175e-02],\n",
            "          [ 1.0516e-02,  3.3765e-02,  3.0490e-03],\n",
            "          [-3.0794e-02,  1.1454e-02,  2.0599e-02]],\n",
            "\n",
            "         [[-8.7486e-03, -5.1234e-03,  2.4214e-02],\n",
            "          [-1.5887e-02, -7.2606e-03, -1.5424e-03],\n",
            "          [ 5.9788e-03,  6.9252e-03,  1.3554e-02]],\n",
            "\n",
            "         [[ 9.1067e-03,  1.4140e-02,  1.6657e-02],\n",
            "          [-2.7136e-02, -3.4461e-02, -8.0477e-03],\n",
            "          [-1.0557e-02, -9.6284e-03,  1.7018e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.1610e-02,  1.9837e-02,  2.0895e-02],\n",
            "          [ 1.9931e-02,  2.3021e-02, -1.2416e-02],\n",
            "          [ 1.5960e-02, -1.9190e-02, -5.2756e-03]],\n",
            "\n",
            "         [[ 3.1193e-02, -1.9346e-02,  3.0284e-03],\n",
            "          [ 4.3195e-03,  1.3865e-02, -2.0477e-02],\n",
            "          [ 4.6717e-03, -2.1775e-02,  7.2630e-03]],\n",
            "\n",
            "         [[ 1.9353e-02, -1.5926e-02, -9.2710e-03],\n",
            "          [ 8.7186e-03,  1.0316e-03,  1.1109e-02],\n",
            "          [-3.4843e-03, -9.0521e-03, -3.7744e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6134e-02, -2.4547e-02,  8.1155e-03],\n",
            "          [ 2.0440e-02, -2.2708e-02,  4.7863e-03],\n",
            "          [ 3.0199e-03,  1.9758e-02,  1.5836e-02]],\n",
            "\n",
            "         [[-6.8383e-03, -3.6529e-02,  1.6275e-02],\n",
            "          [-3.9723e-03, -5.0169e-02, -1.1266e-02],\n",
            "          [-7.1642e-03, -2.6907e-03,  4.5527e-03]],\n",
            "\n",
            "         [[ 1.5249e-02, -1.8749e-02,  1.5318e-02],\n",
            "          [ 4.2624e-02,  3.3101e-02,  2.4634e-02],\n",
            "          [ 3.4689e-02,  8.5061e-03, -3.6524e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.7818e-02, -2.2231e-02, -2.7582e-02],\n",
            "          [-1.7514e-03,  8.1046e-03, -1.3361e-02],\n",
            "          [ 1.9530e-02, -2.3618e-02,  1.9338e-02]],\n",
            "\n",
            "         [[-4.0911e-03, -6.3737e-03,  5.2289e-03],\n",
            "          [-2.5082e-03,  3.9343e-02,  2.8835e-02],\n",
            "          [-1.0883e-02, -1.6624e-02,  2.6649e-02]],\n",
            "\n",
            "         [[-2.5376e-02, -3.8879e-02, -9.3083e-04],\n",
            "          [ 9.5638e-03,  1.9319e-03, -9.7454e-03],\n",
            "          [ 1.6215e-02,  2.8096e-02, -1.1167e-02]]]])), ('conv3.bias', tensor([-0.0050, -0.0281, -0.0074,  0.0180,  0.0108, -0.0291, -0.0099, -0.0223,\n",
            "        -0.0122, -0.0243,  0.0090, -0.0021,  0.0147,  0.0038, -0.0282,  0.0028,\n",
            "         0.0115,  0.0094,  0.0069, -0.0125,  0.0095,  0.0059, -0.0037, -0.0117,\n",
            "         0.0172,  0.0173, -0.0223,  0.0032,  0.0277, -0.0136,  0.0144, -0.0152,\n",
            "         0.0275,  0.0022,  0.0034, -0.0276, -0.0113, -0.0024, -0.0160, -0.0067,\n",
            "         0.0142, -0.0268, -0.0173,  0.0079,  0.0002, -0.0079, -0.0146, -0.0084,\n",
            "         0.0249, -0.0089, -0.0255, -0.0270, -0.0266, -0.0153, -0.0106, -0.0074,\n",
            "        -0.0162,  0.0122, -0.0009, -0.0185, -0.0111, -0.0274,  0.0019,  0.0036,\n",
            "         0.0197, -0.0286,  0.0049, -0.0293, -0.0127, -0.0236,  0.0017,  0.0213,\n",
            "        -0.0145,  0.0233, -0.0173,  0.0079,  0.0074,  0.0172,  0.0262, -0.0242,\n",
            "         0.0196, -0.0191, -0.0164, -0.0146,  0.0052, -0.0104, -0.0283,  0.0229,\n",
            "        -0.0023, -0.0110, -0.0158,  0.0250, -0.0241,  0.0197, -0.0192,  0.0010,\n",
            "        -0.0234, -0.0018, -0.0190, -0.0077, -0.0042, -0.0242, -0.0006,  0.0277,\n",
            "         0.0067,  0.0277, -0.0262,  0.0282, -0.0246,  0.0091, -0.0046,  0.0276,\n",
            "         0.0111,  0.0217,  0.0103, -0.0159, -0.0205, -0.0008,  0.0019,  0.0196,\n",
            "        -0.0271, -0.0240,  0.0221,  0.0022,  0.0139, -0.0091, -0.0043,  0.0094,\n",
            "        -0.0008,  0.0286,  0.0162, -0.0132, -0.0112, -0.0115,  0.0134, -0.0249,\n",
            "         0.0154,  0.0005, -0.0085, -0.0266, -0.0220, -0.0067,  0.0048,  0.0181,\n",
            "         0.0092,  0.0105, -0.0042,  0.0161,  0.0143,  0.0059, -0.0016, -0.0115,\n",
            "         0.0198, -0.0059,  0.0080,  0.0090, -0.0051, -0.0074,  0.0239, -0.0195,\n",
            "        -0.0246,  0.0006,  0.0060,  0.0073,  0.0112,  0.0073, -0.0282, -0.0058,\n",
            "         0.0118, -0.0170, -0.0076,  0.0198,  0.0272, -0.0244,  0.0213, -0.0268,\n",
            "         0.0121,  0.0022,  0.0084,  0.0217,  0.0039, -0.0057,  0.0258, -0.0060,\n",
            "        -0.0228, -0.0144,  0.0154, -0.0200,  0.0188, -0.0284,  0.0170,  0.0222,\n",
            "         0.0217, -0.0271, -0.0245,  0.0185, -0.0176, -0.0116,  0.0142, -0.0006,\n",
            "        -0.0195, -0.0058, -0.0143, -0.0113, -0.0015,  0.0036, -0.0011,  0.0203,\n",
            "        -0.0279,  0.0239,  0.0028, -0.0213, -0.0185, -0.0292, -0.0015, -0.0125,\n",
            "         0.0275, -0.0079,  0.0215, -0.0217, -0.0057,  0.0050, -0.0173,  0.0175,\n",
            "         0.0273,  0.0036,  0.0086, -0.0066, -0.0204,  0.0087,  0.0053, -0.0155,\n",
            "        -0.0056,  0.0117,  0.0092,  0.0264,  0.0100, -0.0172, -0.0010, -0.0058,\n",
            "        -0.0125, -0.0084,  0.0066,  0.0064, -0.0046, -0.0121,  0.0289, -0.0134,\n",
            "        -0.0205, -0.0163,  0.0220,  0.0161, -0.0171, -0.0112, -0.0160, -0.0155])), ('bn3.weight', tensor([0.8994, 1.0166, 0.9062, 0.9381, 0.9849, 0.9029, 0.9351, 0.8980, 0.9646,\n",
            "        1.0037, 0.8976, 0.9219, 0.8829, 0.9520, 1.1228, 0.9646, 0.9670, 0.9462,\n",
            "        1.0470, 0.9530, 0.9341, 0.8866, 1.0586, 0.9595, 0.9275, 0.9214, 0.9554,\n",
            "        0.9958, 0.9259, 0.9309, 0.9315, 1.0506, 0.9871, 0.9102, 0.9865, 0.9199,\n",
            "        0.8760, 1.0126, 1.0061, 0.9332, 0.9155, 1.0769, 0.9464, 0.9888, 0.9179,\n",
            "        1.0110, 0.9857, 0.9401, 0.9605, 1.1246, 0.9415, 0.9061, 0.9898, 0.9040,\n",
            "        1.0471, 0.8971, 0.9432, 0.9966, 0.9164, 1.0002, 1.0088, 0.9350, 0.9244,\n",
            "        0.9417, 0.9764, 0.9720, 0.9281, 0.9261, 0.9269, 0.9877, 0.8989, 1.0298,\n",
            "        0.9157, 0.9798, 0.9898, 0.9458, 0.9208, 0.9630, 0.9821, 0.9262, 0.9242,\n",
            "        0.9608, 1.0582, 0.9617, 0.9456, 1.0302, 0.9679, 0.9200, 0.9787, 0.9166,\n",
            "        0.9183, 0.9132, 0.9282, 0.9628, 0.9665, 0.9348, 0.9609, 0.8747, 0.9949,\n",
            "        0.9131, 1.0009, 0.9439, 0.9475, 0.9589, 0.9390, 0.9671, 0.9883, 0.8958,\n",
            "        0.9055, 0.9348, 0.9861, 0.9213, 0.9199, 0.9971, 0.9394, 0.9382, 0.9484,\n",
            "        0.9640, 0.9381, 0.9138, 0.9653, 0.9170, 0.9285, 0.8979, 1.0228, 0.9345,\n",
            "        0.9195, 1.0032, 0.9685, 0.9470, 0.9432, 0.9325, 0.9195, 0.9231, 1.0157,\n",
            "        0.9164, 0.9413, 0.8974, 1.0266, 0.9126, 0.8890, 0.9526, 0.9597, 0.9197,\n",
            "        0.9380, 0.9755, 1.0167, 1.0105, 0.9846, 0.9690, 1.0041, 0.9705, 0.9492,\n",
            "        0.9072, 0.9424, 0.9485, 0.9531, 0.9104, 0.9233, 0.9620, 0.9283, 0.9408,\n",
            "        0.9085, 0.8973, 0.9353, 1.0153, 0.9332, 0.9431, 0.9326, 0.9466, 0.9408,\n",
            "        0.8971, 0.9656, 0.9752, 0.9447, 1.0156, 1.1279, 0.9386, 0.9007, 0.8910,\n",
            "        0.9229, 1.0414, 0.8985, 0.9558, 0.9078, 0.9166, 0.9327, 0.9320, 0.9058,\n",
            "        0.9263, 0.9346, 1.0081, 0.9019, 0.9041, 0.8979, 0.9777, 0.9647, 0.9469,\n",
            "        0.9669, 0.9404, 1.0078, 0.9565, 0.9874, 1.0212, 0.8891, 0.9455, 0.9574,\n",
            "        0.9028, 0.9183, 0.9203, 0.9294, 0.9111, 0.9109, 0.9198, 0.9665, 0.9467,\n",
            "        0.9431, 0.9085, 0.9552, 0.9001, 0.9694, 0.9158, 0.8804, 0.9605, 0.9472,\n",
            "        0.8939, 0.9076, 0.9419, 0.9789, 1.0124, 1.0360, 0.9911, 0.9884, 0.9286,\n",
            "        0.9565, 1.0015, 0.9817, 0.9085, 0.9418, 0.9188, 0.9518, 0.9634, 1.0468,\n",
            "        0.8864, 0.9332, 0.9082, 1.0013, 0.9488, 1.0084, 0.9427, 0.9191, 0.9069,\n",
            "        0.9113, 0.9832, 0.9040, 0.9155])), ('bn3.bias', tensor([-0.1977, -0.1821, -0.1839, -0.1974, -0.2361, -0.1877, -0.1866, -0.1860,\n",
            "        -0.2114, -0.2246, -0.1814, -0.1623, -0.1682, -0.2027, -0.2243, -0.2204,\n",
            "        -0.1991, -0.1892, -0.2078, -0.2047, -0.2172, -0.1801, -0.2322, -0.1798,\n",
            "        -0.2077, -0.2064, -0.2485, -0.2234, -0.1866, -0.1991, -0.1990, -0.2889,\n",
            "        -0.2222, -0.1499, -0.2477, -0.2013, -0.1666, -0.2458, -0.2483, -0.2421,\n",
            "        -0.1797, -0.2888, -0.1933, -0.2327, -0.1946, -0.1957, -0.2427, -0.2205,\n",
            "        -0.1946, -0.3064, -0.1808, -0.1774, -0.1714, -0.1648, -0.2322, -0.1669,\n",
            "        -0.2418, -0.2462, -0.2199, -0.2085, -0.2500, -0.2014, -0.2151, -0.2005,\n",
            "        -0.1911, -0.1958, -0.2001, -0.1936, -0.1874, -0.2212, -0.1796, -0.2077,\n",
            "        -0.1899, -0.1905, -0.2059, -0.1745, -0.1624, -0.2154, -0.2412, -0.1939,\n",
            "        -0.2159, -0.2128, -0.2239, -0.2039, -0.2044, -0.2604, -0.1853, -0.2056,\n",
            "        -0.1866, -0.2160, -0.2163, -0.1727, -0.2018, -0.2022, -0.2005, -0.1868,\n",
            "        -0.1893, -0.1738, -0.1745, -0.1769, -0.2482, -0.2346, -0.1973, -0.1906,\n",
            "        -0.1970, -0.2295, -0.2321, -0.1792, -0.1725, -0.2015, -0.2102, -0.1807,\n",
            "        -0.1803, -0.2030, -0.2190, -0.1965, -0.1735, -0.2097, -0.1876, -0.1718,\n",
            "        -0.2066, -0.2001, -0.1758, -0.1809, -0.2224, -0.1463, -0.2086, -0.2447,\n",
            "        -0.1830, -0.2328, -0.1853, -0.2321, -0.1579, -0.2237, -0.2137, -0.1632,\n",
            "        -0.2184, -0.1563, -0.2090, -0.2199, -0.2112, -0.1539, -0.1982, -0.1786,\n",
            "        -0.2000, -0.2336, -0.2443, -0.1972, -0.2301, -0.1984, -0.1896, -0.2080,\n",
            "        -0.1924, -0.1866, -0.1581, -0.1777, -0.2387, -0.1818, -0.1979, -0.1908,\n",
            "        -0.1807, -0.2226, -0.2107, -0.1960, -0.2164, -0.2316, -0.1785, -0.1823,\n",
            "        -0.1938, -0.1851, -0.2192, -0.2103, -0.1531, -0.2174, -0.2232, -0.2076,\n",
            "        -0.3145, -0.1715, -0.2048, -0.1695, -0.2057, -0.2420, -0.1717, -0.2071,\n",
            "        -0.1965, -0.2002, -0.2078, -0.1781, -0.1793, -0.2039, -0.2168, -0.2655,\n",
            "        -0.1821, -0.1790, -0.1772, -0.2053, -0.2038, -0.2130, -0.1856, -0.2003,\n",
            "        -0.1858, -0.1907, -0.2624, -0.1739, -0.1956, -0.2395, -0.2118, -0.1882,\n",
            "        -0.2236, -0.1901, -0.2023, -0.1728, -0.1901, -0.1689, -0.1824, -0.1856,\n",
            "        -0.2228, -0.1872, -0.2097, -0.1715, -0.2576, -0.1818, -0.1827, -0.2003,\n",
            "        -0.2333, -0.1745, -0.1915, -0.1776, -0.2041, -0.2627, -0.2135, -0.2582,\n",
            "        -0.2014, -0.1893, -0.1949, -0.2322, -0.1928, -0.1795, -0.1868, -0.2053,\n",
            "        -0.2004, -0.2345, -0.2988, -0.1998, -0.2269, -0.2173, -0.2110, -0.1778,\n",
            "        -0.2178, -0.2096, -0.2163, -0.1974, -0.1779, -0.2282, -0.1542, -0.1903])), ('bn3.running_mean', tensor([ 1.6905e-01, -6.7168e-01, -8.6295e-01, -6.7521e-01,  2.7263e-01,\n",
            "        -9.6641e-01, -1.2483e-01, -1.3872e+00, -6.9120e-01, -1.1700e+00,\n",
            "        -8.0216e-02, -4.4691e-01, -9.0484e-01, -3.4825e-01, -3.9686e-01,\n",
            "        -9.7038e-01, -9.4068e-01, -8.6708e-01, -4.9775e-01, -1.0170e+00,\n",
            "         5.2313e-02, -1.1556e+00,  9.6986e-01,  5.1011e-01, -4.5602e-01,\n",
            "        -9.4468e-01,  4.4541e-01,  5.1041e-01, -1.4142e+00, -1.2585e+00,\n",
            "         4.0964e-01, -4.2495e-01,  5.6846e-01, -4.9599e-02, -4.3300e-01,\n",
            "        -5.7385e-01, -3.0622e-01, -4.3539e-01, -4.9375e-01,  2.4407e-01,\n",
            "        -1.1073e+00, -9.1932e-01, -2.6624e-01, -1.7625e-01, -7.5189e-01,\n",
            "        -9.9498e-01, -1.3613e+00,  4.9106e-01, -5.4610e-01, -4.4444e-02,\n",
            "        -1.0603e+00, -1.1377e+00, -4.9200e-01, -3.7336e-01,  7.3526e-01,\n",
            "        -7.3993e-01,  6.3022e-01, -1.8486e-02,  8.3545e-01, -1.2135e+00,\n",
            "        -8.4700e-02,  5.2903e-02, -5.3784e-01, -1.9468e-01, -1.3807e+00,\n",
            "        -1.5363e+00, -6.1394e-02,  1.1344e-01, -1.7556e+00, -1.5359e+00,\n",
            "        -8.1609e-01, -1.2336e+00, -7.4185e-01, -1.3005e+00, -1.5440e+00,\n",
            "        -3.4543e-01,  2.5080e-01,  6.5079e-01,  1.7140e-01, -1.0231e+00,\n",
            "        -1.9553e-01, -5.6682e-01, -5.5134e-01, -6.1547e-01, -2.5768e+00,\n",
            "        -1.2418e+00, -2.3214e+00,  1.2828e+00, -8.9980e-01, -4.1791e-01,\n",
            "         1.3170e-01, -1.2872e+00,  5.5289e-02, -1.2949e+00,  1.6058e+00,\n",
            "        -1.7707e+00, -1.3275e+00, -1.2929e+00, -8.9276e-01, -4.5963e-01,\n",
            "         2.2758e-01, -8.4181e-01, -1.4824e+00, -2.1050e+00, -1.6849e+00,\n",
            "         1.4793e+00,  6.8461e-01, -1.8474e+00, -1.7620e+00, -2.4715e-01,\n",
            "         3.7615e-01, -6.0950e-01, -7.5258e-01, -1.5546e+00,  6.8906e-01,\n",
            "        -1.9919e+00,  3.7614e-01, -9.9493e-01, -7.8253e-02, -7.1545e-01,\n",
            "        -8.0481e-01, -1.3447e+00, -1.0077e+00, -2.9030e-02, -1.4680e+00,\n",
            "        -7.8477e-01, -1.1931e-01,  3.0189e-01, -3.3279e-01,  6.7214e-01,\n",
            "        -6.6994e-01, -8.4236e-01, -1.8924e+00, -4.1093e-01, -2.0724e-01,\n",
            "        -1.6313e+00, -8.7262e-01, -1.4455e-02,  4.0796e-01,  4.6370e-01,\n",
            "        -1.9076e+00, -8.9626e-01, -2.0842e+00, -1.6670e+00, -6.0708e-01,\n",
            "        -2.9344e-01,  1.8136e-01, -1.3496e+00, -6.7348e-01, -2.5394e-01,\n",
            "         2.3063e-01, -1.1520e+00, -6.4095e-01, -3.1409e-01, -1.2105e+00,\n",
            "        -1.2206e+00,  7.5471e-01, -1.4717e+00, -1.0004e-01, -5.0650e-01,\n",
            "        -1.8440e+00, -8.1917e-01, -4.3722e-01, -8.7194e-01, -3.7059e-01,\n",
            "         8.8765e-01, -1.9210e+00, -2.2249e+00, -2.8006e-01, -3.4511e-01,\n",
            "         2.0262e-01,  1.5437e+00, -2.6715e+00, -9.1445e-01, -2.3356e-01,\n",
            "        -1.9462e+00, -4.1053e-01,  8.1171e-02, -1.0918e+00, -4.1136e-01,\n",
            "        -1.1407e+00,  1.0451e-01, -5.2694e-01,  5.7935e-01,  7.6280e-01,\n",
            "        -1.2994e+00,  7.7864e-01, -7.8205e-01, -2.7830e-01, -1.3250e+00,\n",
            "         8.1193e-02,  6.1194e-01, -6.2774e-01, -2.1477e-01, -7.9760e-01,\n",
            "        -4.6627e-02, -7.1362e-01, -7.1642e-01, -8.0699e-01, -6.3257e-01,\n",
            "        -9.7300e-01,  1.8984e-01,  1.7074e-01, -7.5729e-01,  1.5901e-01,\n",
            "         2.3333e-01, -3.1389e-01,  6.2615e-01,  1.2846e+00,  7.0518e-02,\n",
            "         1.8375e-01, -1.5658e+00, -1.1354e+00, -1.6138e+00, -1.1765e+00,\n",
            "        -9.1679e-01,  8.7636e-01, -9.4038e-05, -1.6163e+00, -1.6635e+00,\n",
            "         1.7263e-01, -9.9273e-01, -5.5221e-01, -1.1657e-01, -3.6877e-01,\n",
            "        -1.7701e+00, -7.7440e-01, -9.2824e-01,  8.2341e-01,  7.6787e-01,\n",
            "        -1.7672e+00, -5.4156e-01,  5.2002e-01, -8.6848e-01, -3.6767e-01,\n",
            "         1.6672e-01, -2.9355e+00, -1.0629e+00, -1.6646e-01, -4.0312e-01,\n",
            "        -8.9350e-01, -3.5080e-01,  5.4658e-01, -1.2989e-01,  4.9549e-01,\n",
            "         7.1015e-01, -1.1884e+00, -1.6485e+00, -9.8315e-01, -5.7105e-02,\n",
            "         1.5601e-01, -4.2812e-01, -5.9673e-01, -1.3278e+00,  5.5761e-02,\n",
            "        -1.1181e+00])), ('bn3.running_var', tensor([0.6513, 0.7060, 1.0781, 0.5282, 0.6765, 1.1480, 0.7999, 0.8019, 1.0896,\n",
            "        0.8481, 0.8351, 0.7778, 0.6665, 0.6968, 1.5584, 0.6914, 0.6989, 0.7442,\n",
            "        1.1581, 0.6530, 0.7346, 0.6422, 1.3930, 1.0671, 0.8949, 0.6739, 0.6215,\n",
            "        0.9982, 0.9688, 0.6476, 0.8884, 0.7724, 0.7826, 0.9679, 0.7652, 0.9752,\n",
            "        1.0360, 0.8223, 0.6929, 0.7186, 0.6150, 0.9996, 0.7399, 0.7494, 0.6442,\n",
            "        0.9835, 0.7783, 0.8085, 0.9124, 0.7378, 1.0368, 0.7730, 0.9724, 0.6493,\n",
            "        0.9626, 0.8854, 0.7634, 0.8677, 0.7051, 0.9461, 1.1695, 0.8361, 0.7403,\n",
            "        0.8082, 0.7684, 0.9747, 0.6756, 0.7850, 0.7112, 0.7520, 0.8483, 1.2722,\n",
            "        0.7300, 0.8661, 0.8149, 1.2124, 0.6836, 0.8494, 0.6919, 0.5668, 0.5763,\n",
            "        0.6693, 1.1829, 0.8561, 0.7615, 0.9585, 0.7972, 0.7105, 1.0252, 0.6945,\n",
            "        0.6799, 0.7223, 1.0155, 0.7956, 0.9012, 0.7330, 0.6537, 0.8841, 1.1319,\n",
            "        0.7137, 1.0802, 0.8852, 0.7445, 0.8562, 0.7152, 0.9077, 0.9450, 0.7535,\n",
            "        0.4795, 0.7898, 0.7539, 0.9560, 0.6178, 0.5872, 0.5779, 0.7287, 0.7050,\n",
            "        0.9345, 0.5959, 0.9151, 1.0308, 0.5896, 0.6960, 0.7005, 0.7541, 1.0498,\n",
            "        0.5276, 0.8520, 0.7819, 0.6943, 0.7206, 0.9845, 0.8573, 0.6912, 1.1465,\n",
            "        0.7600, 0.9163, 0.6439, 0.7908, 0.5351, 0.5998, 0.6433, 0.7473, 0.6678,\n",
            "        0.5145, 0.7155, 1.0805, 0.7417, 0.7270, 0.6901, 0.7665, 0.9106, 1.0229,\n",
            "        0.6582, 0.9561, 0.8916, 0.6315, 0.8295, 0.7749, 0.7386, 0.9473, 0.6414,\n",
            "        0.9821, 0.9570, 0.6645, 0.8446, 1.1405, 0.7090, 0.5742, 0.7603, 0.9486,\n",
            "        0.6810, 0.9297, 1.1944, 0.6460, 1.0611, 1.2624, 0.8007, 0.6921, 0.7222,\n",
            "        0.7594, 0.8544, 0.6047, 1.0132, 0.6746, 0.7270, 0.8073, 0.9365, 0.8352,\n",
            "        0.5376, 0.6674, 0.9292, 0.6421, 0.7173, 0.7081, 0.8160, 0.8287, 0.6099,\n",
            "        0.7494, 0.7606, 1.1466, 0.7705, 0.7023, 0.9404, 0.8469, 1.0960, 0.8123,\n",
            "        0.4831, 0.6078, 1.0430, 0.9043, 0.8806, 0.5815, 0.8129, 0.7054, 1.1045,\n",
            "        0.6009, 0.5211, 0.7520, 0.9586, 0.8763, 1.0276, 0.6561, 1.2190, 0.6141,\n",
            "        0.6914, 0.6665, 0.7287, 0.8490, 0.8111, 0.8739, 0.9860, 1.1889, 1.0144,\n",
            "        1.0621, 0.7517, 0.8830, 0.8217, 0.7299, 0.5486, 0.7868, 0.6558, 0.7745,\n",
            "        0.7140, 0.6615, 0.6984, 0.8304, 1.0326, 1.1986, 0.5384, 0.6053, 0.7601,\n",
            "        0.6756, 1.2884, 0.6869, 0.8721])), ('bn3.num_batches_tracked', tensor(5100)), ('fc1.weight', tensor([[-0.0073, -0.0076, -0.0060,  ..., -0.0056,  0.0085, -0.0116],\n",
            "        [ 0.0137,  0.0097, -0.0059,  ..., -0.0071, -0.0087, -0.0141],\n",
            "        [ 0.0270, -0.0110,  0.0082,  ...,  0.0151,  0.0038, -0.0033],\n",
            "        ...,\n",
            "        [-0.0118, -0.0070,  0.0100,  ..., -0.0109, -0.0046, -0.0155],\n",
            "        [-0.0151,  0.0105, -0.0026,  ...,  0.0069,  0.0126,  0.0036],\n",
            "        [-0.0157, -0.0151,  0.0093,  ..., -0.0069, -0.0135, -0.0134]])), ('fc1.bias', tensor([-0.0107,  0.0042, -0.0040,  ...,  0.0029,  0.0061,  0.0028])), ('fc2.weight', tensor([[-1.6445e-02, -1.8173e-02, -2.0753e-02,  ..., -2.8317e-03,\n",
            "          4.1223e-03,  3.0053e-02],\n",
            "        [-2.1368e-02, -2.3869e-02,  4.5334e-02,  ..., -3.5522e-03,\n",
            "         -3.0697e-02,  1.7144e-02],\n",
            "        [-2.6571e-02, -4.7809e-05,  4.0527e-03,  ..., -2.6165e-02,\n",
            "         -1.5566e-02,  1.4136e-02],\n",
            "        ...,\n",
            "        [-2.2725e-02, -4.8161e-03,  1.2837e-02,  ...,  8.8333e-03,\n",
            "         -8.8995e-03,  2.5101e-02],\n",
            "        [-2.2678e-02, -2.5981e-02, -1.7312e-02,  ..., -1.8817e-02,\n",
            "         -2.7212e-02, -1.1000e-02],\n",
            "        [ 1.4135e-02,  1.3867e-02, -2.0009e-02,  ...,  2.5036e-02,\n",
            "          4.7183e-03,  2.1760e-02]])), ('fc2.bias', tensor([-0.0251, -0.0159,  0.0037,  0.0145, -0.0274,  0.0026,  0.0201, -0.0173,\n",
            "         0.0323, -0.0160,  0.0154,  0.0386, -0.0109, -0.0138,  0.0356,  0.0296,\n",
            "        -0.0148,  0.0077,  0.0051,  0.0182,  0.0235,  0.0023,  0.0537, -0.0159,\n",
            "         0.0148, -0.0068, -0.0328, -0.0255,  0.0066,  0.0411,  0.0259,  0.0270,\n",
            "        -0.0052, -0.0054, -0.0148, -0.0108, -0.0210, -0.0218, -0.0099,  0.0082,\n",
            "         0.0281,  0.0312,  0.0289, -0.0103,  0.0464,  0.0288,  0.0039,  0.0293,\n",
            "         0.0551,  0.0239,  0.0141,  0.0158, -0.0110,  0.0139, -0.0098,  0.0095,\n",
            "        -0.0255,  0.0650, -0.0004,  0.0115,  0.0165,  0.0170, -0.0015, -0.0202,\n",
            "         0.0354,  0.0261,  0.0190,  0.0069,  0.0089,  0.0617, -0.0186,  0.0004,\n",
            "         0.0585, -0.0105,  0.0061, -0.0255,  0.0137, -0.0210,  0.0457,  0.0050,\n",
            "         0.0283, -0.0049, -0.0167,  0.0344,  0.0221,  0.0043,  0.0475,  0.0294,\n",
            "         0.0729,  0.0117,  0.0209,  0.0085,  0.0088,  0.0091,  0.0212,  0.0329,\n",
            "         0.0100,  0.0324,  0.0492,  0.0065,  0.0228,  0.0069,  0.0174, -0.0051,\n",
            "        -0.0245, -0.0039, -0.0176,  0.0252,  0.0281,  0.0045, -0.0277,  0.0339,\n",
            "         0.0405,  0.0063,  0.0514,  0.0459,  0.0450, -0.0143,  0.0336,  0.0175,\n",
            "        -0.0002, -0.0186,  0.0191, -0.0119, -0.0225,  0.0063, -0.0233, -0.0036,\n",
            "         0.0396,  0.0329,  0.0273, -0.0003,  0.0239,  0.0019,  0.0066,  0.0215,\n",
            "         0.0689,  0.0038,  0.0339, -0.0319,  0.0158,  0.0237,  0.0172,  0.0641,\n",
            "         0.0019,  0.0102, -0.0229, -0.0132,  0.0219,  0.0203,  0.0239, -0.0197,\n",
            "        -0.0132, -0.0155,  0.0225,  0.0158,  0.0249,  0.0283,  0.0393, -0.0242,\n",
            "         0.0496,  0.0219,  0.0304,  0.0382, -0.0099,  0.0333,  0.0054,  0.0178,\n",
            "        -0.0095,  0.0129, -0.0009,  0.0179,  0.0202,  0.0489, -0.0184,  0.0223,\n",
            "         0.0079,  0.0260,  0.0173,  0.0433,  0.0190,  0.0115, -0.0111,  0.0211,\n",
            "        -0.0238,  0.0489, -0.0054, -0.0109,  0.0259,  0.0015,  0.0616, -0.0027,\n",
            "         0.0330, -0.0099,  0.0094,  0.0497,  0.0179,  0.0189, -0.0112,  0.0184,\n",
            "         0.0317, -0.0208, -0.0272,  0.0493,  0.0513, -0.0192,  0.0330, -0.0129,\n",
            "         0.0349,  0.0319,  0.0117,  0.0074,  0.0485, -0.0160,  0.0359, -0.0105,\n",
            "        -0.0035,  0.0370,  0.0275, -0.0017, -0.0326,  0.0232,  0.0779, -0.0042,\n",
            "         0.0156, -0.0224,  0.0228,  0.0011,  0.0144,  0.0128,  0.0290,  0.0141,\n",
            "        -0.0304, -0.0143,  0.0379, -0.0155,  0.0686,  0.0345,  0.0549,  0.0512,\n",
            "         0.0019,  0.0060,  0.0148,  0.0092, -0.0333, -0.0006,  0.0027,  0.0217,\n",
            "        -0.0226, -0.0146,  0.0039,  0.0193,  0.0390, -0.0138,  0.0205,  0.0110,\n",
            "         0.0247,  0.0057,  0.0015,  0.0313,  0.0076, -0.0123, -0.0006,  0.0009,\n",
            "         0.0146, -0.0017,  0.0006,  0.0525, -0.0293, -0.0082, -0.0278, -0.0072,\n",
            "         0.0557,  0.0113,  0.0338,  0.0264, -0.0267,  0.0645, -0.0319, -0.0236,\n",
            "         0.0578, -0.0392,  0.0292,  0.0208, -0.0001,  0.0349,  0.0366,  0.0083,\n",
            "        -0.0051,  0.0541,  0.0106,  0.0244,  0.0337,  0.0292, -0.0261,  0.0178,\n",
            "         0.0496, -0.0237,  0.0371, -0.0202,  0.0440,  0.0403, -0.0184,  0.0370,\n",
            "         0.0502,  0.0342,  0.0597, -0.0288,  0.0213, -0.0054,  0.0347, -0.0258,\n",
            "         0.0292,  0.0370,  0.0126,  0.0382, -0.0137,  0.0496, -0.0319, -0.0082,\n",
            "         0.0270, -0.0108,  0.0142, -0.0029,  0.0017, -0.0130, -0.0118,  0.0490,\n",
            "         0.0348,  0.0185,  0.0178,  0.0072,  0.0048,  0.0289,  0.0218, -0.0341,\n",
            "         0.0165,  0.0157, -0.0022, -0.0385,  0.0122,  0.0020, -0.0074, -0.0106,\n",
            "         0.0097, -0.0075,  0.0029, -0.0054, -0.0084,  0.0301,  0.0303,  0.0380,\n",
            "         0.0159,  0.0128,  0.0346,  0.0064,  0.0385,  0.0745, -0.0104,  0.0255,\n",
            "         0.0162,  0.0098,  0.0211, -0.0281, -0.0034,  0.0031,  0.0314,  0.0106,\n",
            "         0.0198, -0.0005, -0.0190, -0.0157,  0.0328, -0.0188, -0.0165,  0.0143,\n",
            "        -0.0234,  0.0262,  0.0424,  0.0277,  0.0178,  0.0139,  0.0338,  0.0304,\n",
            "         0.0779, -0.0008, -0.0013,  0.0603,  0.0148,  0.0251,  0.0160, -0.0219,\n",
            "         0.0280,  0.0158,  0.0109, -0.0138,  0.0303, -0.0222,  0.0487, -0.0287,\n",
            "         0.0288,  0.0457,  0.0306, -0.0107, -0.0059,  0.0012,  0.0150, -0.0200,\n",
            "        -0.0049,  0.0435, -0.0204,  0.0531,  0.0368,  0.0058, -0.0148,  0.0476,\n",
            "         0.0226,  0.0284,  0.0153, -0.0129, -0.0013,  0.0456,  0.0169,  0.0019,\n",
            "         0.0182,  0.0017,  0.0248,  0.0710, -0.0140,  0.0144,  0.0124,  0.0541,\n",
            "         0.0347,  0.0314,  0.0065, -0.0256,  0.0506, -0.0122,  0.0148,  0.0029,\n",
            "         0.0140,  0.0791,  0.0049, -0.0161,  0.0526,  0.0201, -0.0055, -0.0124,\n",
            "        -0.0119, -0.0166, -0.0007, -0.0030,  0.0130, -0.0040,  0.0206, -0.0110,\n",
            "        -0.0133,  0.0226,  0.0347, -0.0002, -0.0119,  0.0074, -0.0325,  0.0348,\n",
            "         0.0015,  0.0056, -0.0387, -0.0145,  0.0380,  0.0221, -0.0019,  0.0659,\n",
            "         0.0106, -0.0047,  0.0280,  0.0246, -0.0016, -0.0351,  0.0159, -0.0055,\n",
            "         0.0762,  0.0500,  0.0344,  0.0240, -0.0137, -0.0151,  0.0220, -0.0303,\n",
            "        -0.0242,  0.0064, -0.0124, -0.0199, -0.0163,  0.0023, -0.0021,  0.0586,\n",
            "        -0.0095,  0.0547, -0.0353,  0.0183, -0.0135,  0.0170, -0.0042, -0.0200,\n",
            "        -0.0035,  0.0271, -0.0263,  0.0051,  0.0356,  0.0413,  0.0281,  0.0357])), ('fc3.weight', tensor([[ 0.0280,  0.0626,  0.0258,  ...,  0.0682,  0.0372, -0.1043],\n",
            "        [-0.0340,  0.0170, -0.1276,  ..., -0.1156,  0.0216, -0.1180],\n",
            "        [-0.0036,  0.0585, -0.0175,  ...,  0.0832, -0.0116,  0.1045],\n",
            "        ...,\n",
            "        [-0.0436, -0.0879, -0.0257,  ..., -0.0676, -0.0053,  0.0419],\n",
            "        [-0.0394, -0.0043,  0.0102,  ..., -0.0113, -0.0223, -0.0249],\n",
            "        [-0.0437, -0.0047, -0.0871,  ..., -0.1308,  0.0111, -0.0470]])), ('fc3.bias', tensor([ 0.0566, -0.2630, -0.0912,  0.3728,  0.1476, -0.1719, -0.0046,  0.0683,\n",
            "        -0.1899, -0.0439]))])\n"
          ]
        }
      ]
    }
  ]
}